<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>python爬虫学习（一）</title>
    <url>/2025/04/13/python%E7%88%AC%E8%99%AB/</url>
    <content><![CDATA[1.前言本篇开始想要从基础的爬虫案例开始，逐步完成爬虫相关技术的学习，在本篇中，我会用东方财富 、51游戏、中国人事考试网等案例进行，从易到难。
2.爬虫功能分类在进行案例之前先说明一下爬虫的功能分类，一共有如下几种

通用爬虫：直接对页面的所有数据进行爬取
聚焦爬虫：对页面中的数据有选择性的爬取
功能爬虫：通过浏览器或者app实现自动化爬取
增量式爬虫：对新更新的数据进行补充爬取，以前爬取的数据不再新爬取
分布式爬虫：搭建分布式集群对网络资源进行联合且分布的爬取当然本篇的案例只是很基础的案例，甚至代码都不会多

3.1 东方财富网网址：https://www.eastmoney.com/
import requests url = &#x27;https://www.eastmoney.com/&#x27;#向指定URL进行请求，响应数据response = requests.get(url =url) response.encoding = &#x27;utf-8&#x27;page = response.text #text返回的是字符串的响应数据with open(&#x27;dongfang.html&#x27;,&#x27;w&#x27;) as fp: #如果返回还有编码问题，在open中也加个 encoding=&#x27;utf-8&#x27; 	fp.write(page) #这里的page是上述获取的响应数据
通过运行上方代码即可得到，下述内容的文件在右上角四个浏览器中选择一个进入即可看到爬取成果
3.2 51游戏网网址：https://www.51.com/上一个案例中，只是简单的对url的首页进行通用爬取，但事实上你进入一个网站肯定不会只因为其主页，比如这个游戏网，你若想搜集带有“王者”这个关键词的游戏仅靠先前给出代码无法实现该需求此时可以先进入网页去搜索看看相关的网址会是什么样通过类似输入多个关键词，会发现url前面https://game.51.com/search/action/game/都没有改变，只有后面的&quot;王者&quot;这个关键词变化了，所以对应的代码也会对应变化
import requests    game_title = input(&#x27;请输入想要搜索的关键词：&#x27;)  #存放需要的请求参数params = &#123;    &#x27;q&#x27;: game_title  &#125;    url= &quot;https://www.51.com/&quot;  #这是搜索的网址头first_url = &#x27;https://game.51.com/search/action/game/&#x27;  response = requests.get(url=first_url,params=params)  response.encoding = &#x27;utf-8&#x27;  page_51 = response.text    with open(&#x27;51.html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fp:      fp.write(page_51)

3.3 中国人事考试网
网址 http://www.cpta.com.cn/如果你根据先前的代码去进行爬取，并不会报错，但是当你将爬取到的网址打开以后并不会得到对应网址，而是如下图的报错。

于是据此，提出一个说法，对于此类问题都是由于爬虫模拟浏览器的程度不够，网站认为我的这个id不正常，不属于正常用户访问浏览器的id所以无法进行爬取，所以，我们需要在代码中加入header，也就是请求头，你可以通过在浏览器中按F12，进入开发者模式请求标头的就是正常用户提出请求会携带的数据，而先前的代码中并没有这些，所谓的反爬机制就是这样找到你模拟不够完善的地方，然后组织爬虫代码访问，但并不是说请求头中的所有都需要填进去，多数仅需要UA和cookie就可以使得代码正常运行(对应数据按照上述图去找到复制即可)
import requests  url = &#x27;http://www.cpta.com.cn/&#x27;  header = &#123;      &#x27;user-agent&#x27;:&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&#x27;  &#125;    response = requests.get(url=url,headers=header)  response.encoding= &#x27;utf-8&#x27;  page_cpta = response.text  with open(&#x27;cpta.html&#x27;,&#x27;w&#x27;,encoding= &#x27;utf-8&#x27;) as f:      f.write(page_cpta)
当然获取的文件中可能会只包含数据，一些网站的设计排版不一定会有，但数据爬到就可以了如果我们想要更进一步搜索，比如想搜“人力资源”相关的信息，肯定还需要另外的代码，在写之前可以去实际网页搜索试试，看看网址有什么关系你会发现这个页面和之前的51游戏页面有有所不同这里的网址值只有search，无论你搜什么关键词，网址都只是这个，此时你可以打开F12，查看相关信息，你会发现与之前的请求方式不同，这个页面采用的是post请求方式post数据往往会返回数据存储在json文件中，所以你应当去找对应的数据，也就是标头右边的载荷将这里的数据复制到代码中再进行爬虫就可以达成效果
import requests  url = &#x27;http://www.cpta.com.cn/category/search&#x27;  key_word = input(&#x27;请输入想要搜索的关键词：&#x27;)  params = &#123;        &#x27;keywords&#x27;: key_word ,      &#x27;搜 索&#x27;: &#x27;搜 索&#x27;  &#125;  header = &#123;  &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&#x27;,  &#125;  response = requests.post(url= url,headers=header, params=params)  response.encoding=&#x27;utf=8&#x27;  page_cpta = response.text  with open(&#x27;cpta_search.html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as f:      f.write(page_cpta)
4. KFC餐厅
网址：http://www.kfc.com.cn/kfccda/storelist/index.aspx在这里你若是想要通过关键词去进行查询你会发现，它并不会挑跳转到新网址，但是你若是按原本网址来却只能获取到无搜索时的数据。这是因为在页面数据中有一种数据名为动态数据，动态加载数据值不是直接通过浏览器地址栏的url请求到的数据，这些数据叫做动态加载数据。那么怎么判别数据是否是动态数据呢？你可以通过开发者模式，进入到当前页面的network查找窗口可以通过ctrl+f 进行呼出，输入关键词，然后在响应中去搜索关键词看看是否能找到对应的数据，若无贼说明是动态数据可以通过动态数据左上角的全局搜索进行找到对应文件动态数据往往会用json存储，所以写代码的时候得留意，并且数据往往不止一页，有多页，需要采取多页连续采取import requests  key_word = input(&#x27;请输入关键词:&#x27;)  page_input = input(&#x27;请输入页码(如1-5或1,2,3)：&#x27;)    pages=[]  if &#x27;-&#x27; in page_input:      start,end = map(int, page_input.split(&#x27;-&#x27;))      pages= list(range(start, end+1))  else:      pages = [int(page_input)]    with open(&#x27;kfc1.txt&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as f:      f.write(&quot;门店名称\t地址\n&quot;)  # 写入表头      for page in pages:          url = &#x27;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#x27;          header = &#123;          &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&#x27;          &#125;          data = &#123;              &#x27;cname&#x27;:&#x27;&#x27; ,               &#x27;pid&#x27;: &#x27;&#x27;,              &#x27;keyword&#x27;: key_word,              &#x27;pageIndex&#x27;: page,              &#x27;pageSize&#x27;: &#x27;10&#x27;          &#125;            response = requests.post(url=url,headers=header,data=data)          response.encoding= &#x27;utf-8&#x27;          page_KFC = response.json()            for detail in page_KFC[&#x27;Table1&#x27;]:              storename = detail.get(&#x27;storeName&#x27;, &#x27;N/A&#x27;)              add = detail.get(&#x27;addressDetail&#x27;, &#x27;N/A&#x27;)              f.write(f&quot;&#123;storename&#125;\t&#123;add&#125;\n&quot;)              print(f&quot;成功保存：&#123;storename&#125;-&#123;add&#125;&quot;)

]]></content>
  </entry>
  <entry>
    <title>python爬虫学习（三）</title>
    <url>/2025/04/16/%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[tip
要是觉得找elements中对应部分有点麻烦，可以用ctrl+shift+c然后点网页你想要的部分，开发者模式中就会跳转过去了。

1.防盗链
现在很多网站启用了防盗链反爬，防止服务器上的资源被人恶意盗取。什么是防盗链呢？

从HTTP协议说起，在HTTP协议中，有一个表头字段：referer，采用URL的格式来表示从哪一个链接跳转到当前网页的。通俗理解就是：客户端的请求具体从哪里来，服务器可以通过referer进行溯源。一旦检测来源不是网页所规定的，立即进行阻止或者返回指定的页面。



2. 三里屯抓拍图爬取网址：http://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1可以尝试按先前的方式先去爬取试试，你会发现无法爬取想要的照片，当你用开发者模式去看的时候，你会发现她有两个url，你尝试两个src会发现两个都得不到正确的图片，这就是防盗链的作用，所以我们需要再header中加入referer那么如何去找到referer呢?你需要在network中找到要爬取的图片，可以通过预览迅速辨认是否自己要找的图片，具体可按下图：
import requests  from lxml import etree  url = &#x27;http://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1&#x27;  header = &#123;      &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&#x27;,      &#x27;referer&#x27;:&#x27;https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1&#x27;      # &quot;Referer&quot;: &quot;http://blog.sina.com.cn/&quot;  &#125;  response = requests.get(url=url,headers=header)  response.encoding=&#x27;utf-8&#x27;  page_img= response.text  tree = etree.HTML(page_img)  img_list = tree.xpath(&#x27;//*[@id=&quot;sina_keyword_ad_area2&quot;]/div/a/img/@real_src&#x27;)  for img in img_list:      data = requests.get(url=img,headers=header).content      with open(&#x27;name.jpg&#x27;,&#x27;wb&#x27;) as f:          f.write(data)      break

3. cookie
什么是cookie？
cookie的本质就是一组数据（键值对的形式存在）
是由服务器创建，返回给客户端，最终会保存在客户端浏览器中。
如果客户端保存了cookie，则下次再次访问该服务器，就会携带cookie进行网络访问。
典型的案例：网站的免密登录





tip在通过的之前学习后，相信对于爬虫代码的基础部分已经有所掌握，对于这些开源的数据在有爬虫技术前真要获取也是可以的，比如说你一个个人工去搜集得到，但这种机械化又大量的工作正是促使爬虫技术的诞生，更进一步，一些爬虫的技术代码是否也是如此呢？据此，我想要介绍一个工具‘curlconverter’网址：https://curlconverter.com/通过上述步骤再复制的网站中，就可得到如下一些基础的代码，也可以提高一定的效率。
案例：雪球网数据#ps：用谷歌和edge可能会无法用开发者模式获取数据，可能是一种反爬机制，但博主用联想浏览器可以打开，并且代码可以成功运行。网址：https://xueqiu.com/如上图一样找到curl，然后到之前的给出的网站获得对应的代码再进行修改即可，代码如下：
import time    import requests  import json  pages = input(&#x27;请你输入想要爬取几页：&#x27;)  cookies = &#123;      &#x27;xq_a_token&#x27;: &#x27;9773bacc11404cb5ac8b0847c564eda3730e6b61&#x27;,      &#x27;xqat&#x27;: &#x27;9773bacc11404cb5ac8b0847c564eda3730e6b61&#x27;,      &#x27;xq_r_token&#x27;: &#x27;caf75c71460c20a680e6e3c455cadcca5c1be14a&#x27;,      &#x27;xq_id_token&#x27;: &#x27;eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJ1aWQiOi0xLCJpc3MiOiJ1YyIsImV4cCI6MTc0NzM1NzE4MywiY3RtIjoxNzQ1MDQ4NDMxOTI5LCJjaWQiOiJkOWQwbjRBWnVwIn0.S6DfPNTKxGAQwLt9gFPf2A16vqu5_UCPLrr5qgtlUuGnbtmGSsahB6kSe_bfpCbwvNmGgoDQ6Ot2vVDQtXwJfqWXNS8pKzskKHhed8L48G4uV0g1iX4dmrWqHFfZwwCVn3YKEH2H7dP20eU_U1FTXC8ikqHtjXPxu661rmTMGVdKG3xD5uM8DDcwZUBC6ZpetUUtzTe5To_8prNyBFujdfuUsd8zsaS-1gRCgqyiRWgQxAq204mmgo7uJ7TDWjwfYnSvg0m_JMYW8Wj7MV5ajR2dRM3XBEcNFtOWYNQi0xWhY13vt-NDZLKBWNE4mkirNtjG5cdjTO03ATJ6eYDYhg&#x27;,      &#x27;cookiesu&#x27;: &#x27;401745048477218&#x27;,      &#x27;u&#x27;: &#x27;401745048477218&#x27;,      &#x27;device_id&#x27;: &#x27;e67adc9306236a61a4c97845cd9ebec4&#x27;,      &#x27;Hm_lvt_1db88642e346389874251b5a1eded6e3&#x27;: &#x27;1745048506&#x27;,      &#x27;HMACCOUNT&#x27;: &#x27;A39F3B65D44D61BF&#x27;,      &#x27;Hm_lpvt_1db88642e346389874251b5a1eded6e3&#x27;: &#x27;1745048777&#x27;,      &#x27;ssxmod_itna&#x27;: &#x27;QqUOGKAK0KD5YKGQGCDhZILq7QGO1D7IDkDl4BtGRDeq7tDRDFqApYDHKjwzdsYWD1Axx4tFmAeR5DBMxbDSxD6ODK4GTpIoeWXPeDpBxIIrYQgrXorDcmeM5qwjer+pjsL/6MzpDCPGnD06x+ALbDYYLDBYD74G+DDeDi23Dj4GmDGAdNeDFGnb=gebPTxDwDB=DmkwWYPDfDDdYsBYiGlcVIeeD0qmsxo4Q+DGWGQuVW=p=DGUxRWQYx0UFDBLef/FDGu8jxguQsFQ+HbrWrPGuDG=HXQ0=jORiyli+do+qF4VbB34TK7tO7e=G4q7x1gDcfx+YqpYh80QTGGID=CE+pDDi2X=RpqiAb7Un2tYgtBa4TnG9mbg0eUeKilP7/Dd9wsmGGQe+WKHthQCD4eheWA/=xxD&#x27;,      &#x27;ssxmod_itna2&#x27;: &#x27;QqUOGKAK0KD5YKGQGCDhZILq7QGO1D7IDkDl4BtGRDeq7tDRDFqApYDHKjwzdsYWD1Axx4tFmAebDia4WEINhSngwzVbeCY2YeI3afD&#x27;,  &#125;    headers = &#123;      &#x27;accept&#x27;: &#x27;application/json, text/plain, */*&#x27;,      &#x27;accept-language&#x27;: &#x27;zh-CN,zh;q=0.9&#x27;,      &#x27;origin&#x27;: &#x27;https://xueqiu.com&#x27;,      &#x27;priority&#x27;: &#x27;u=1, i&#x27;,      &#x27;referer&#x27;: &#x27;https://xueqiu.com/&#x27;,      &#x27;sec-ch-ua&#x27;: &#x27;&quot;Chromium&quot;;v=&quot;9&quot;, &quot;Not?A_Brand&quot;;v=&quot;8&quot;&#x27;,      &#x27;sec-ch-ua-mobile&#x27;: &#x27;?0&#x27;,      &#x27;sec-ch-ua-platform&#x27;: &#x27;&quot;Windows&quot;&#x27;,      &#x27;sec-fetch-dest&#x27;: &#x27;empty&#x27;,      &#x27;sec-fetch-mode&#x27;: &#x27;cors&#x27;,      &#x27;sec-fetch-site&#x27;: &#x27;same-site&#x27;,      &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 SLBrowser/9.0.6.2081 SLBChan/103 SLBVPV/64-bit&#x27;,  &#125;  l = []  for page in pages:      params = &#123;          &#x27;page&#x27;: page,          &#x27;size&#x27;: &#x27;10&#x27;,          &#x27;type&#x27;: &#x27;sha&#x27;,          &#x27;order_by&#x27;: &#x27;percent&#x27;,          &#x27;order&#x27;: &#x27;desc&#x27;,      &#125;        response = requests.get(          &#x27;https://stock.xueqiu.com/v5/stock/screener/quote/list.json&#x27;,          params=params,          cookies=cookies,          headers=headers,      )      d = response.json().get(&#x27;data&#x27;).get(&#x27;list&#x27;)      # print(d)        l.extend(d)      # print(l)      #确保不要访问频率过快    time.sleep(1)    with open(&#x27;list.json&#x27;,&#x27;w&#x27;) as fp:      fp.write(json.dumps(l, indent=2, ensure_ascii=False))

但是虽然目前的cookie都能直接用，但实际上在主流的网站，对于cookie可能是有防备的，比如cookie可能会有有效期，或者说cookie就是一次性的，当这种情况下，对于，这样获取的cookie可能就无法成功用于代码中正如先前所言，所有爬虫代码的不成功都是拟人度不够，比如说现在的代码都是根据请求头和网址直接进去数据所在的页面，但是正常流程应该是打开网站的首页，再点击之后的，所以通过代码先对主页面进行访问，浏览器会给代码一个全新的cookie，代码通过这个cookie就可以更稳定运行了
import requests  # 第一次请求  import requests    headers = &#123;      &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7&#x27;,      &#x27;Accept-Language&#x27;: &#x27;zh-CN,zh;q=0.9&#x27;,      &#x27;Cache-Control&#x27;: &#x27;no-cache&#x27;,      &#x27;Connection&#x27;: &#x27;keep-alive&#x27;,      &#x27;Pragma&#x27;: &#x27;no-cache&#x27;,      &#x27;Sec-Fetch-Dest&#x27;: &#x27;document&#x27;,      &#x27;Sec-Fetch-Mode&#x27;: &#x27;navigate&#x27;,      &#x27;Sec-Fetch-Site&#x27;: &#x27;none&#x27;,      &#x27;Sec-Fetch-User&#x27;: &#x27;?1&#x27;,      &#x27;Upgrade-Insecure-Requests&#x27;: &#x27;1&#x27;,      &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&#x27;,      &#x27;sec-ch-ua&#x27;: &#x27;&quot;Chromium&quot;;v=&quot;130&quot;, &quot;Google Chrome&quot;;v=&quot;130&quot;, &quot;Not?A_Brand&quot;;v=&quot;99&quot;&#x27;,      &#x27;sec-ch-ua-mobile&#x27;: &#x27;?0&#x27;,      &#x27;sec-ch-ua-platform&#x27;: &#x27;&quot;macOS&quot;&#x27;,  &#125;  url = &#x27;https://xueqiu.com/hq&#x27;  response = requests.get(url, headers=headers)  # print(response.status_code)  # print(response.headers)  # print(response.content)  # print(response.text)  print(response.cookies.get_dict())  c1 = response.cookies.get_dict()    # 第二次请求，携带动态cookie  headers = &#123;      &#x27;accept&#x27;: &#x27;application/json, text/plain, */*&#x27;,      &#x27;accept-language&#x27;: &#x27;zh-CN,zh;q=0.9&#x27;,      &#x27;cache-control&#x27;: &#x27;no-cache&#x27;,      &#x27;origin&#x27;: &#x27;https://xueqiu.com&#x27;,      &#x27;pragma&#x27;: &#x27;no-cache&#x27;,      &#x27;priority&#x27;: &#x27;u=1, i&#x27;,      &#x27;referer&#x27;: &#x27;https://xueqiu.com/&#x27;,      &#x27;sec-ch-ua&#x27;: &#x27;&quot;Chromium&quot;;v=&quot;130&quot;, &quot;Google Chrome&quot;;v=&quot;130&quot;, &quot;Not?A_Brand&quot;;v=&quot;99&quot;&#x27;,      &#x27;sec-ch-ua-mobile&#x27;: &#x27;?0&#x27;,      &#x27;sec-ch-ua-platform&#x27;: &#x27;&quot;macOS&quot;&#x27;,      &#x27;sec-fetch-dest&#x27;: &#x27;empty&#x27;,      &#x27;sec-fetch-mode&#x27;: &#x27;cors&#x27;,      &#x27;sec-fetch-site&#x27;: &#x27;same-site&#x27;,      &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&#x27;,  &#125;    params = &#123;      &#x27;page&#x27;: &#x27;1&#x27;,      &#x27;size&#x27;: &#x27;10&#x27;,      &#x27;type&#x27;: &#x27;sha&#x27;,      &#x27;order_by&#x27;: &#x27;percent&#x27;,      &#x27;order&#x27;: &#x27;desc&#x27;,      &#x27;md5__1632&#x27;: &#x27;7q+xuQKDqmwOD/Wi4BKw1QDcWODO7fAeD&#x27;,  &#125;    response = requests.get(      &#x27;https://stock.xueqiu.com/v5/stock/screener/quote/list.json&#x27;,      params=params,      cookies=c1,      headers=headers,  )    print(response.text)

 基于session对象实现自动处理cookie。

1.创建一个空白的session对象。
2.需要使用session对象发起请求，请求的目的是为了捕获cookie    - 注意：如果session对象在发请求的过程中，服务器端产生了cookie，则cookie会自动存储在session对象中。
3.使用携带cookie的session对象，对目的网址发起请求，就可以实现携带cookie的请求发送，从而获取想要的数据。

注意：session对象至少需要发起两次请求

第一次请求的目的是为了捕获存储cookie到session对象
后次的请求，就是携带cookie发起的请求了import requests  # 第一次请求  import requests  session = requests.session()    headers = &#123;      &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7&#x27;,      &#x27;Accept-Language&#x27;: &#x27;zh-CN,zh;q=0.9&#x27;,      &#x27;Cache-Control&#x27;: &#x27;no-cache&#x27;,      &#x27;Connection&#x27;: &#x27;keep-alive&#x27;,      &#x27;Pragma&#x27;: &#x27;no-cache&#x27;,      &#x27;Sec-Fetch-Dest&#x27;: &#x27;document&#x27;,      &#x27;Sec-Fetch-Mode&#x27;: &#x27;navigate&#x27;,      &#x27;Sec-Fetch-Site&#x27;: &#x27;none&#x27;,      &#x27;Sec-Fetch-User&#x27;: &#x27;?1&#x27;,      &#x27;Upgrade-Insecure-Requests&#x27;: &#x27;1&#x27;,      &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&#x27;,      &#x27;sec-ch-ua&#x27;: &#x27;&quot;Chromium&quot;;v=&quot;130&quot;, &quot;Google Chrome&quot;;v=&quot;130&quot;, &quot;Not?A_Brand&quot;;v=&quot;99&quot;&#x27;,      &#x27;sec-ch-ua-mobile&#x27;: &#x27;?0&#x27;,      &#x27;sec-ch-ua-platform&#x27;: &#x27;&quot;macOS&quot;&#x27;,  &#125;  url = &#x27;https://xueqiu.com/hq&#x27;  # session：自动管理session  session.get(url, headers=headers)        # 第二次请求，携带动态cookie  headers = &#123;      &#x27;accept&#x27;: &#x27;application/json, text/plain, */*&#x27;,      &#x27;accept-language&#x27;: &#x27;zh-CN,zh;q=0.9&#x27;,      &#x27;cache-control&#x27;: &#x27;no-cache&#x27;,      #&#x27;cookie&#x27;: &#x27;u=851695628862640; HMACCOUNT=2A4FB6ADBAE70C94; cookiesu=591728562312333; device_id=ce2bbbc0e3a672b689052fc9204f8b61; xq_a_token=7716f523735d1e47a3dd5ec748923068ab8198a8; xqat=7716f523735d1e47a3dd5ec748923068ab8198a8; xq_r_token=0483ea3986e45954e03c9294444a1af14d7579d3; xq_id_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJ1aWQiOi0xLCJpc3MiOiJ1YyIsImV4cCI6MTczMzEwMDgxNywiY3RtIjoxNzMxNzY2NTA1ODkyLCJjaWQiOiJkOWQwbjRBWnVwIn0.GqQPJ3wNkRxoWlb8W32s66nnlZCr9_8QhlChq5S2hdVOPBA4JcZFM2d9XaxkULzYt9KxFnDKa4dx_t4BDWfK-Bm9pV5YiheFBwvePRwnFldsSgVuw-SJBHTthJCo3wOzFkxubRyFtC248rKmuNPSaD1x7QtTHFk77O-5917lxJlip61oEZ-o1c51N31vGcQfxjzK2Do1xwPaU-3MZK_-jL3wuMZEcbRCbBTpD1GQj7PrATbajsO4Z1x7Xf58CmrLHeBz6DXIhNDf28MkxoEjSzT-mSNsCEIMXp0dbSQqhEp0D1hX3LqcCKLHebCRTiR0dMPMMIR1E_0a4-swY9Goag; Hm_lvt_1db88642e346389874251b5a1eded6e3=1731766508; is_overseas=0; ssxmod_itna=iqGxyDBiitG=KAIK0dGQDHYySeewx7IXIKAmYedND/SGIDnqD=GFDK40EoSxPd+3iYDxR3hGl7A4+F1/n8GwFoFx+OpxXSQxiTD4q07Db4GkDAqiOD7uqhoD445GwD0eG+DD4DWzqDU/fhKDjGHCcp5HAOIHfh5DbhKODiK8DYvpDAThVPYzCcxzDDapDlKhDWPODQHsAxKDExGOX+ImBxGaFfSLmbKDEj+Cmb3DvcOOG12z/xY8/txYeWpkU6nePchYYjixGn0Dzn293EDyF704dDrP3r0FkrZeee3lzYD=; ssxmod_itna2=iqGxyDBiitG=KAIK0dGQDHYySeewx7IXIKAmYedG9WMtDBL80D7prMqaG2YzxFqG7eeD; Hm_lpvt_1db88642e346389874251b5a1eded6e3=1731766579&#x27;,      &#x27;origin&#x27;: &#x27;https://xueqiu.com&#x27;,      &#x27;pragma&#x27;: &#x27;no-cache&#x27;,      &#x27;priority&#x27;: &#x27;u=1, i&#x27;,      &#x27;referer&#x27;: &#x27;https://xueqiu.com/&#x27;,      &#x27;sec-ch-ua&#x27;: &#x27;&quot;Chromium&quot;;v=&quot;130&quot;, &quot;Google Chrome&quot;;v=&quot;130&quot;, &quot;Not?A_Brand&quot;;v=&quot;99&quot;&#x27;,      &#x27;sec-ch-ua-mobile&#x27;: &#x27;?0&#x27;,      &#x27;sec-ch-ua-platform&#x27;: &#x27;&quot;macOS&quot;&#x27;,      &#x27;sec-fetch-dest&#x27;: &#x27;empty&#x27;,      &#x27;sec-fetch-mode&#x27;: &#x27;cors&#x27;,      &#x27;sec-fetch-site&#x27;: &#x27;same-site&#x27;,      &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36&#x27;,  &#125;    params = &#123;      &#x27;page&#x27;: &#x27;1&#x27;,      &#x27;size&#x27;: &#x27;10&#x27;,      &#x27;type&#x27;: &#x27;sha&#x27;,      &#x27;order_by&#x27;: &#x27;percent&#x27;,      &#x27;order&#x27;: &#x27;desc&#x27;,      &#x27;md5__1632&#x27;: &#x27;7q+xuQKDqmwOD/Wi4BKw1QDcWODO7fAeD&#x27;,  &#125;    response = session.get(      &#x27;https://stock.xueqiu.com/v5/stock/screener/quote/list.json&#x27;,      params=params,      headers=headers,  )    print(response.text)

]]></content>
  </entry>
  <entry>
    <title>python爬虫学习（二）</title>
    <url>/2025/04/15/%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[题外话
爬虫相关案例不会详细到一步步放出来，比如获取多个页面的数据，你就去多看几个页面的网址，大概率就能发现他的规律，又或者先获取当前页面的跳转网址，在对获取到的网址再做一次类似的爬取，很多就是一步步轮下去的，而我呈现的是完整的一步到位的代码，所以感觉有跳跃或者有问题，可以勤用print，可以较为直观的看出是哪里有问题。

1、数据解析上篇中的案例其实主要都是在进行通用爬虫，但对于用户来说，爬取整个页面的信息是冗余的，许多信息是无用数据甚至还需要用户自行去挑选，需要耗费大量的时间和精力，所以数据解析就显得无可或缺。
数据解析的概念：可以将爬取到的数据中指定的数据进行单独提取作用：实现聚焦爬虫通用原理：

在一张页面中，爬取到的数据往往存储于html文件中
html文件中，可以通过标签去定位想要获取的数据位置，并进行提取数据解析技术：
xpath（通用性最强）
bs4（python独有，学习成本低）
正则表达式（复杂度高）
pyquery（css语句）这次主要以xpath技术为主，xpath的编码流程：
创建一个etree类型的对象，把被解析的数据加载到该对象中
调用etree对象中的xpath函数结合不同形式的xpath表达式进行数据提取

2.案例一通过新建一个text.html文件，对html进行分析来初步理解xpath进行数据解析的原理
&lt;html lang=&quot;en&quot;&gt;     &lt;head&gt;     	&lt;meta charset=&quot;UTF-8&quot; /&gt;     	&lt;title&gt;测试bs4&lt;/title&gt;     &lt;/head&gt;     &lt;body&gt;     	&lt;div&gt;     		&lt;p&gt;百里守约&lt;/p&gt;     	&lt;/div&gt;     	&lt;div class=&quot;song&quot;&gt;     		&lt;p&gt;李清照&lt;/p&gt;     		&lt;p&gt;王安石&lt;/p&gt;     		&lt;p&gt;苏轼&lt;/p&gt;     		&lt;p&gt;柳宗元&lt;/p&gt;     		&lt;a href=&quot;http://www.song.com/&quot; title=&quot;赵匡胤&quot; target=&quot;_self&quot;&gt;     			&lt;span&gt;this is span&lt;/span&gt;     		宋朝是最强大的王朝，不是军队的强大，而是经济很强大，国民都很有钱&lt;/a&gt;     		&lt;a href=&quot;&quot; class=&quot;du&quot;&gt;总为浮云能蔽日,长安不见使人愁&lt;/a&gt;     		&lt;img src=&quot;http://www.baidu.com/meinv.jpg&quot; alt=&quot;&quot; /&gt;     	&lt;/div&gt;     	&lt;div class=&quot;tang&quot;&gt;     		&lt;ul&gt;     			&lt;li&gt;&lt;a href=&quot;http://www.baidu.com&quot; title=&quot;qing&quot;&gt;清明时节雨纷纷,路上行人欲断魂,借问酒家何处有,牧童遥指杏花村&lt;/a&gt;&lt;/li&gt;     			&lt;li&gt;&lt;a href=&quot;http://www.163.com&quot; title=&quot;qin&quot;&gt;秦时明月汉时关,万里长征人未还,但使龙城飞将在,不教胡马度阴山&lt;/a&gt;&lt;/li&gt;     			&lt;li&gt;&lt;a href=&quot;http://www.126.com&quot; alt=&quot;qi&quot;&gt;岐王宅里寻常见,崔九堂前几度闻,正是江南好风景,落花时节又逢君&lt;/a&gt;&lt;/li&gt;     			&lt;li&gt;&lt;a href=&quot;http://www.sina.com&quot; class=&quot;du&quot;&gt;杜甫&lt;/a&gt;&lt;/li&gt;     			&lt;li&gt;&lt;a href=&quot;http://www.dudu.com&quot; class=&quot;du&quot;&gt;杜牧&lt;/a&gt;&lt;/li&gt;     			&lt;li&gt;&lt;b&gt;杜小月&lt;/b&gt;&lt;/li&gt;     			&lt;li&gt;&lt;i&gt;度蜜月&lt;/i&gt;&lt;/li&gt;     			&lt;li&gt;&lt;a href=&quot;http://www.haha.com&quot; id=&quot;feng&quot;&gt;凤凰台上凤凰游,凤去台空江自流,吴宫花草埋幽径,晋代衣冠成古丘&lt;/a&gt;&lt;/li&gt;     		&lt;/ul&gt;     	&lt;/div&gt;     &lt;/body&gt;     &lt;/html&gt;
代码中我们需要使用一个新的包lxml,可以通过下述在终端中进行下载
pip install lxml
from lxml import etree    #先创建一个etree对象，把数据加载到etree中去  tree = etree.parse(&#x27;test.html&#x27;)    #对数据进行提取  #xpath函数返回的是列表，列表中存储的是满足定位要求的所有标签  #具体获取html下的head中的title  ret = tree.xpath(&#x27;/html/head/title&#x27;)  #   ‘//’获取所有的title  ret1 = tree.xpath(&#x27;//title&#x27;)    #对&lt;body&gt;中的数据进行提取,你会发现有多个&lt;div&gt;  ret2 = tree.xpath(&#x27;//div&#x27;) #获取所有的div  ret3 = tree.xpath(&#x27;//div[@class=&quot;song&quot;]&#x27;) #获取class为song的数据  ret4 = tree.xpath(&#x27;//div[@class=&quot;song&quot;]/p&#x27;)#获取该div下的p标签数据    #运行之后你会发现他给的都是&lt;Element p at 0x2555a90de80&gt;这样的数据  #如果要获得文本数据，则需要使用text()  ret5 = tree.xpath(&#x27;//a[@id=&quot;feng&quot;]/text()&#x27;) #提取对应a中的数据  ret6 = tree.xpath(&#x27;//a//text()&#x27;) #提取a下所有的文本数据  #总结：/text（）提取对应下的文本，//text()提取所有的文本    #提取标签的属性值，比如图片地址 //tag/@attrNameret7 = tree.xpath(&#x27;//img/@src&#x27;)  print(ret7)
3. 碧血剑小说网址：https://www.jinyongwang.net/bi/结果：将各个章节的标题和内容进行爬取然后存储到文件中先找到对应的标签的位置然后右键即可复制xpath，减少工作量
import requests  from lxml import etree  from urllib.parse import urljoin    url =&#x27;https://www.jinyongwang.net/bi/&#x27;  header = &#123;  &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&#x27;,  &#x27;referer&#x27;:&#x27;https://www.jinyongwang.net/bi/&#x27;,  &#x27;cookie&#x27;: &#x27;PHPSESSID=t880k0dcvvjkbgdulcaa25krf6&#x27;  &#125;  response = requests.get(url=url,headers=header)  response.encoding=&#x27;utf-8&#x27;  page_bi = response.text  # 上述代码是和之前差不多的  #但与前一个案例不同，因为这是requests得到数据不是html文件所以不是parse  tree = etree.HTML(page_bi)    # f = open(&#x27;bixuejian.txt&#x27;,&#x27;w&#x27;)  with open(&#x27;bixuejian.txt&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as f:      li_list = tree.xpath(&#x27;//*[@id=&quot;pu_box&quot;]/div[3]/ul/li&#x27;)          for li in li_list:          #局部标签，只在li中寻找,因为xpath得到的是列表，          # 所以还得[0]是为了得到字符串          title = li.xpath(&#x27;./a/text()&#x27;)[0]          detail = li.xpath(&#x27;./a/@href&#x27;)[0]          detail_url = urljoin(url, detail)          res = requests.get(url=detail_url,headers=header )          res.encoding= &#x27;utf-8&#x27;          content = res.text          detail_tree = etree.HTML(content)          text = detail_tree.xpath(&#x27;//*[@id=&quot;vcon&quot;]//p//text()&#x27;)          text_join  = &#x27;,\n&#x27;.join(text).strip()          # print(text_join)          # f.write(title+&#x27;:&#x27;+ text_join+&#x27;\n&#x27;)        f.write(f&quot;&#123;title&#125;: &#123;text_join&#125;\n&quot;)          print(title + &#x27;  下载成功&#x27;)
上述就是结果样式
4. 简历爬取网址&#x3D; https://sc.chinaz.com/jianli/下述就是需要得到的xpath
import requests  from lxml import etree    page_input = input(&#x27;请输入搜索页码（1-5）：&#x27;)    pages=[]  if &#x27;-&#x27; in page_input:      start,end = map(int, page_input.split(&#x27;-&#x27;))      pages= list(range(start, end+1))  else:      pages = [int(page_input)]    for page in (1,2):      if page == 1:          url = &#x27;https://sc.chinaz.com/jianli/index.html&#x27;      else:          url = &#x27;https://sc.chinaz.com/jianli/index_%d.html&#x27;%page      print(&#x27;当前正在爬取第%d页的数据&#x27;%page)      header = &#123;      &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36&#x27;,        &#125;      response = requests.get(url=url, headers=header)      response.encoding=&#x27;utf-8&#x27;      page_jianli = response.text      tree = etree.HTML(page_jianli)      div_list = tree.xpath(&#x27;//*[@id=&quot;container&quot;]/div&#x27;)      for div in div_list:          title = div.xpath(&#x27;./p/a/text()&#x27;)[0]          detail_url = div.xpath(&#x27;./a/@href&#x27;)[0]          jianli_response = requests.get(url=detail_url,headers=header)          jianli_response.encoding= &#x27;utf-8&#x27;          page_down = jianli_response.text          #创建etree对象，将数据存入对象中          down_tree = etree.HTML(page_down)          #通过标签定位获取数据          down_url = down_tree.xpath(&#x27;//*[@id=&quot;down&quot;]/div[2]/ul/li[1]/a/@href&#x27;)[0]          down_response = requests.get(url=down_url,headers=header)          down_content = down_response.content          name = &#x27;./jianli/&#x27;+ title+ &#x27;.rar&#x27;          with open(name,&#x27;wb&#x27;) as f:              f.write(down_content)

ps（pthon）：

序列化：json.dumps()
反序列化：json.load()

]]></content>
  </entry>
  <entry>
    <title>利用DrissionPge爬取淘宝数据</title>
    <url>/2025/04/20/%E5%88%A9%E7%94%A8DrissionPge%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[本篇文章将利用DrissionPage完成淘宝商品数据爬取DrissionPage官网：https://drissionpage.cn/
首先需要下载DrissionPage
pip install DrissionPage
下载好之后首先要做的就是设置运行的浏览器
from DrissionPage import ChromiumOptions  #这是我自己的谷歌浏览器的地址path = r&quot;C:\Program Files\Google\Chrome\Application\chrome.exe&quot;    ChromiumOptions().set_browser_path(path).save()
这个代码运行成功后，之后都不用运行了
1. 登录页面像淘宝，京东等较为大型的电商服务平台，进入网站后还需要登录之后才能进行搜索，所以我们需要现在登录页面进行操作网址：https://login.taobao.com/havanaone/login/login.htm复制账号输入框css格式的定位，以此类推，密码输入框和登录按钮的定位也如此获得
from DrissionPage import ChromiumPage  from cookie import name,password  import time  #模拟浏览器  dp = ChromiumPage()   #进入页面   url = &#x27;https://login.taobao.com/havanaone/login/login.htm&#x27;   dp.get(url)     #定位账号输入端，并输入账号   dp.ele(&#x27;css:#fm-login-id&#x27;).input(name)   #定位密码输入端，并输入密码   dp.ele(&#x27;css:#fm-login-password&#x27;).input(password)   time.sleep(2)   #定位登录按钮并点击   dp.ele(&#x27;css:#login-form &gt; div.fm-btn &gt; button&#x27;).click()
在代码中，你会发现这个账号密码的获取方式可能不清楚，实际上是通过自己重新建一个py文件新建完后再里面输入
name = &#x27;xxx&#x27;  #xxx换成你自己的账号密码password = &#x27;xxx&#x27;
然后根据自己建立的py文件的名称，导入到主文件中即可
from 你的文件名.py import name, password
2.爬取页面登录页面的代码在运行后也可除dp赋值外其他均可注释，因为运行过后，会在一定时间内无需登录。

在爬取页面中就要获得对应的数据。但按照b站的上的视频走，在这一步会有些问题，因为我发现在淘宝首页进行搜索，在以前可能是页面刷新，而现在是重新打开一页，这会导致dp仍然定位在首页，所以无法得到搜索后新开的页面的数据。所以我就尝试修改，直接跳转到搜索后的新页面。
from DrissionPage import ChromiumPage  from cookie import name,password  import time  import re  import json  from DataRecorder import Recorderkey_word = &#x27;手机&#x27;  #定位淘宝首页搜索框并输入  dp.get(&#x27;https://s.taobao.com/search?page=1&amp;q=&#x27;+ key_word + &#x27;&amp;tab=all&#x27;)  time.sleep(2)    #监听  dp.listen.start(&#x27;h5/mtop.relationrecommend.wirelessrecommend.recommend/2.0&#x27;)response = dp.listen.wait().responseprint(response.body)mtopjsonp = response.body  #用正则将需要的数据提取出来  mtopjson = re.findall(&#x27;mtopjsonp\d+\((.*)\)&#x27;, mtopjsonp)[0]  print(mtopjson)
上述代码中的监听功能，即跳转到该页面时对后面这地址（也就是我们所需要的数据）进行获取。那么这个数据应该怎么找呢？很简单，你随便复制一个商品的标题，部分即可，进入开发者模式，全局搜索就能找到对应的url，不需要全部复制，与我这种类似即可。通过上述代码得到的结果与开发者工具中preview上的一致
3.数据解析#反序列化  mtop_data = json.loads(mtopjson)  itemarray = mtop_data.get(&#x27;data&#x27;).get(&#x27;itemsArray&#x27;)  data_array = []#遍历获取数据  for item in itemarray:      title = item.get(&#x27;title&#x27;)      price = item.get(&#x27;price&#x27;)      realSales = item.get(&#x27;realSales&#x27;)      procity =item.get(&#x27;procity&#x27;)      nick = item.get(&#x27;nick&#x27;)#店铺      auctionURL = item.get(&#x27;auctionURL&#x27;)      pic_path = item.get(&#x27;pic_path&#x27;)      print(title,price,realSales,procity,nick,auctionURL,pic_path)	data_array.append([title,price,realSales,procity,nick,auctionURL,pic_path])

4.数据存储通过DataRecorder进行存储
from DataRecorder import Recorder  r = Recorder(&#x27;手机.xlsx&#x27;)r.add_data(data_array)  r.record()

5.整体代码（含批量存储）批量存储通过for循环，同样通过定位到“下一页”的按钮进行翻页
from DrissionPage import ChromiumPage  from cookie import name,password  import time  import re  import json  from DataRecorder import Recorder    r = Recorder(&#x27;手机.xlsx&#x27;)  data_array = []  header = [&#x27;title&#x27;, &#x27;price&#x27;, &#x27;realSales&#x27;, &#x27;procity&#x27;, &#x27;nick&#x27;, &#x27;auctionURL&#x27;, &#x27;pic_path&#x27;]  data_array.append(header)    #模拟浏览器  dp = ChromiumPage()  # #进入页面  # url = &#x27;https://login.taobao.com/havanaone/login/login.htm&#x27;  # dp.get(url)    # #定位账号输入端，并输入账号  # dp.ele(&#x27;css:#fm-login-id&#x27;).input(name)  # #定位密码输入端，并输入密码  # dp.ele(&#x27;css:#fm-login-password&#x27;).input(password)  # time.sleep(2)  # #定位登录按钮并点击  # dp.ele(&#x27;css:#login-form &gt; div.fm-btn &gt; button&#x27;).click()  key_word = &#x27;手机&#x27;  #定位淘宝首页搜索框并输入  # dp.get(&#x27;https://s.taobao.com/search?page=1&amp;q=%E6%B8%B8%E6%88%8F%E7%94%B5%E7%AB%9E%E6%89%8B%E6%9C%BA&amp;tab=all&#x27;)  dp.get(&#x27;https://s.taobao.com/search?page=1&amp;q=&#x27;+ key_word + &#x27;&amp;tab=all&#x27;)    time.sleep(2)    #监听  dp.listen.start(&#x27;h5/mtop.relationrecommend.wirelessrecommend.recommend/2.0&#x27;)    dp.refresh()  for p in range(1,3):        #下滑页面      dp.scroll.to_bottom()        try:          #等待加载获取响应体数据          response = dp.listen.wait().response          mtopjsonp = response.body          #用正则将需要的数据提取出来          mtopjson = re.findall(&#x27;mtopjsonp\d+\((.*)\)&#x27;, mtopjsonp)[0]          print(mtopjson)          #反序列化          mtop_data = json.loads(mtopjson)          itemarray = mtop_data.get(&#x27;data&#x27;).get(&#x27;itemsArray&#x27;)            #遍历获取数据          for item in itemarray:              title = item.get(&#x27;title&#x27;)              price = item.get(&#x27;price&#x27;)              realSales = item.get(&#x27;realSales&#x27;)              procity =item.get(&#x27;procity&#x27;)              nick = item.get(&#x27;nick&#x27;)#店铺              auctionURL = item.get(&#x27;auctionURL&#x27;)              pic_path = item.get(&#x27;pic_path&#x27;)              # print(title,price,realSales,procity,nick,auctionURL,pic_path)              #保存运行              data_array.append([title,price,realSales,procity,nick,auctionURL,pic_path])          r.add_data(data_array)          r.record()          time.sleep(3)          dp.ele(&#x27;css:#search-content-leftWrap &gt; div.leftContent--BdYLMbH8 &gt; div.pgWrap--RTFKoWa6 &gt; div &gt; div &gt; button.next-btn.next-medium.next-btn-normal.next-pagination-item.next-next&#x27;).click()      except (AttributeError, ValueError, TypeError, IndexError) as e:          print(f&quot;错误：&#123;e&#125;&quot;)          continue
文件输出如下图
]]></content>
  </entry>
  <entry>
    <title>python爬虫学习（四）</title>
    <url>/2025/04/21/%E7%88%AC%E8%99%AB%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[scrapy框架1. 概念在网络数据采集的领域，Scrapy无疑是一款强大而高效的爬虫框架。它为用户提供了丰富的功能，能够快速抓取和解析网站数据。无论是简单的网页抓取还是复杂的数据提取，Scrapy都能助你一臂之力。



requests
scrapy



页面级爬虫
网站级爬虫


功能库
框架


并发性考虑不足，性能较差
并发性好，性能高


重点在于页面下载
重点在于爬虫结构


定制灵活
一般制定灵活，深度制定困难


上手十分简单
入门稍难


相关图如下：








Spiders：中文可以称为蜘蛛，Spiders 是一个复数的统称，其可以对应多个 Spider，每个 Spider 里面定义了站点的爬取逻辑和页面的解析规则。
Scrapy Engine：引擎，是整个框架的核心。可以理解为整个框架的中央处理器(类似人的大脑)，负责数据的流转和逻辑的处理。
Scheduler：调度器，它用来接受 Engine 发过来的 Request 并将其加入队列中，同时也可以将 Request 发回给 Engine 供 Downloader 执行
Downloader：下载器，完成向服务器发送请求，然后拿到响应的过程，并发送给Egine
Item pipeline：管道，用于处理抽取的数据，例如数据清洗、保存到数据库等。
Item：它是一个抽象的数据结构，所以在图中没有体现出来，它定义了爬取结果的数据结构，爬取的数据会被赋值成 Item 对象。

2.豆瓣电影爬取网址：https://movie.douban.com/先下载scrapy相关
pip install scrapy
下载完成后，先创建一个scrapy项目，在你想要创建的文件夹中cmd打开，并输入
scrapy startproject 你的项目名
之后cmd’中会提示你进行
cd 你的项目名scrapy genspider example example.com #example example.com 也是需要进行修改的，并且不能和项目名一致，且这个注释不能复制到cmd中
创建完项目之后，用pycharm打开即可，之后可以先去分析网址，我认为这一步是尤为重要的，尤其是在scrapy框架此类工具帮助下，代码的书写越来越简单，对网页的分析占比在一次爬虫任务中更为重要。
2.1 items.py通过开发者模式将想要爬取的数据进行定位，确定要爬取哪些信息，然后在items.py中写入对应的对象。
class DoubanItem(scrapy.Item):      rank = scrapy.Field()  # 电影排名      movie_name = scrapy.Field() #电影名称      movie_quote = scrapy.Field()  # 电影名言      picture = scrapy.Field()  # 电影海报链接      movie_rating = scrapy.Field()  # 电影评分      evaluators = scrapy.Field()  # 评价人数
2.2 db.py （自己命名的那个）完成items的代码后，开始常规上的第一步，也就是对你的db.py进行编写代码
name = &quot;db&quot;  allowed_domains = [&quot;movie.douban.com&quot;]  # 允许的域名范围  start_urls = [&quot;https://movie.douban.com/top250&quot;]  # 爬虫从这个 URL 开始抓取
上述这个代码是你在先前scrapy genspider example example.com 时就会默认生成的，可以根据实际需求进行更改。而完成另外一个parse方法则是这个文件中最为关键的一步。parse 是 Scrapy 中的默认解析方法，爬虫启动后，Scrapy 会自动下载 start_urls 中的页面，并将响应传递给这个方法。在这里，我们会使用 XPath 来提取电影信息。（这些与requests的获取响应数据类似）


首先使用 Selector 对页面进行解析。



然后使用 XPath 提取页面中所有电影的列表，遍历每部电影并提取需要的字段信息，如电影的排名、名称、简介、评分等。



最后通过 yield 返回提取到的数据。def parse(self, response): sel = Selector(response) # 使用 Selector 解析 HTML 响应movie_items = sel.xpath(&#x27;//div[@class=&quot;article&quot;]//ol[@class=&quot;grid_view&quot;]/li&#x27;) # 提取所有电影条目
获得页面数据列表后就需要对其进行遍历,根据先前在items中已经建立好的对象from douban.items import DoubanItem#并且留意整个项目代码希望是完整独立的，不要嵌套在别的文件夹中，这会导致引用包时出现错误for movie in movie_items:      item = DoubanItem()  # 创建 item 实例      # 提取电影信息      item[&#x27;rank&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;pic&quot;]/em/text()&#x27;).extract_first()      item[&#x27;movie_name&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;hd&quot;]//span[1]/text()&#x27;).extract_first()      item[&#x27;movie_quote&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;bd&quot;]/p[@class=&quot;quote&quot;]/span/text()&#x27;).extract_first(          default=&#x27;无&#x27;)      item[&#x27;picture&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;pic&quot;]/a/img/@src&#x27;).extract_first()      item[&#x27;movie_rating&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;bd&quot;]/div/span[2]/text()&#x27;).extract_first()      item[&#x27;evaluators&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;bd&quot;]/div/span[4]/text()&#x27;).extract_first()        yield item  # 返回 item
由于这样获得数据仅仅是一页的数据，所以我们还需要进行分页处理，确保获得所有的数据。豆瓣电影的 Top 250 列表分成了 10 页，每页显示 25 部电影。在爬取完第一页后，我们需要继续爬取剩下的页面。通过提取页面底部的“下一页”链接，来实现分页抓取。next_link = sel.xpath(&#x27;//span[@class=&quot;next&quot;]/link/@href&#x27;).extract_first()  if next_link:      yield Request(url=response.urljoin(next_link), callback=self.parse)
如果你习惯于cmd可以不用这个代码，但是如果你习惯于直接用pycharm去运行程序，须在代码中添加from scrapy import cmdlineif __name__ == &#x27;__main__&#x27;:      #所写的‘db’是与上述的name相同      cmdline.execute(&#x27;scrapy crawl db&#x27;.split())



下述为db.py的完整代码
import scrapy  from scrapy import Selector,Request  from scrapy import cmdline  from douban.items import DoubanItem    class DbSpider(scrapy.Spider):      name = &quot;db&quot;      allowed_domains = [&quot;movie.douban.com&quot;]  # 允许的域名范围      start_urls = [&quot;https://movie.douban.com/top250&quot;]  # 爬虫从这个 URL 开始抓取        def parse(self, response):          sel = Selector(response)  # 解析页面          movie_items = sel.xpath(&#x27;//div[@class=&quot;article&quot;]//ol[@class=&quot;grid_view&quot;]/li&#x27;)  # 获取电影条目列表            for movie in movie_items:              item = DoubanItem()  # 创建 item 实例              # 提取电影信息              item[&#x27;rank&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;pic&quot;]/em/text()&#x27;).extract_first()              item[&#x27;movie_name&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;hd&quot;]//span[1]/text()&#x27;).extract_first()              item[&#x27;movie_quote&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;bd&quot;]/p[@class=&quot;quote&quot;]/span/text()&#x27;).extract_first(                  default=&#x27;无&#x27;)              item[&#x27;picture&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;pic&quot;]/a/img/@src&#x27;).extract_first()              item[&#x27;movie_rating&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;bd&quot;]/div/span[2]/text()&#x27;).extract_first()              item[&#x27;evaluators&#x27;] = movie.xpath(&#x27;.//div[@class=&quot;bd&quot;]/div/span[4]/text()&#x27;).extract_first()                yield item  # 返回 item          # 处理分页          next_link = sel.xpath(&#x27;//span[@class=&quot;next&quot;]/link/@href&#x27;).extract_first()          if next_link:              yield Request(url=response.urljoin(next_link), callback=self.parse)        if __name__ == &#x27;__main__&#x27;:      #所写的‘db’是与上述的name相同      cmdline.execute(&#x27;scrapy crawl db&#x27;.split())

2.3 pipelines.py在 Scrapy 中，pipelines.py 文件用于处理爬取到的数据。这里，我们要将抓取到的豆瓣电影数据保存到 Excel 文件中。为了实现这一点，我们会使用 Python 的 openpyxl 库，它是一个专门用于处理 Excel 文件的库。DoubanPipeline类DoubanPipeline 类将处理从爬虫传递过来的数据，并将这些数据写入 Excel 文件。Scrapy 中的 Pipeline 类通常有三个方法：open_spider、process_item 和 close_spider。

**open_spider**：当爬虫启动时调用，一般用于初始化一些资源。
**process_item**：每当爬虫抓取到一个数据项（item）时，都会调用此方法来处理该项数据。
**close_spider**：当爬虫结束时调用，用于保存文件或释放资源。  import openpyxl  # 导入处理 Excel 文件的库  from .items import DoubanItem  # 导入定义好的 Item 数据结构    class DoubanPipeline:      def __init__(self):          &quot;&quot;&quot;          初始化方法，在爬虫开始时被调用，初始化 Excel 工作簿和表格。          &quot;&quot;&quot;        self.wb = openpyxl.Workbook()  # 创建一个新的 Excel 工作簿          self.sheet = self.wb.active  # 获取工作簿的活动表格          self.sheet.title = &#x27;豆瓣电影Top250&#x27;  # 设置表格的标题          # 在第一行写入表头，表示每列的意义          self.sheet.append((&#x27;电影排名&#x27;, &#x27;电影名称&#x27;, &#x27;电影名言&#x27;, &#x27;电影评分&#x27;, &#x27;观影人数&#x27;, &#x27;电影海报&#x27;))        def open_spider(self, spider):          &quot;&quot;&quot;          当爬虫被启动时调用该方法。          :param spider: 当前运行的爬虫对象          &quot;&quot;&quot;        print(&#x27;开始爬虫...&#x27;)  # 可以选择在控制台输出提示信息，表示爬虫开始运行        def process_item(self, item: DoubanItem, spider):          &quot;&quot;&quot;          处理每个爬取到的数据项（item），将其保存到 Excel 文件中。          :param item: 爬取到的电影数据          :param spider: 当前运行的爬虫对象          :return: 返回处理后的 item        &quot;&quot;&quot;        # 将每部电影的信息以一行的形式写入 Excel        self.sheet.append((              item[&#x27;rank&#x27;],  # 电影排名              item[&#x27;movie_name&#x27;],  # 电影名称              item[&#x27;movie_quote&#x27;],  # 电影名言              item[&#x27;movie_rating&#x27;],  # 电影评分              item[&#x27;evaluators&#x27;] ,  # 观影人数              item[&#x27;picture&#x27;] # 电影海报链接            ))          # 返回 item 是 Scrapy Pipeline 的标准流程，方便后续可能有其他 pipeline 处理          return item        def close_spider(self, spider):          &quot;&quot;&quot;          当爬虫关闭时调用，保存 Excel 文件。          :param spider: 当前运行的爬虫对象          &quot;&quot;&quot;        print(&quot;爬虫结束....&quot;)  # 输出爬虫结束的提示          # 保存 Excel 文件到指定路径          self.wb.save(&#x27;豆瓣电影数据.xlsx&#x27;)

2.4 setting.pysetting中是scrapy中的全局配置文件，我会选择出几个较为重要的进行解释说明。
BOT_NAME = &quot;douban&quot;    SPIDER_MODULES = [&quot;douban.spiders&quot;]  NEWSPIDER_MODULE = &quot;douban.spiders&quot;

BOT_NAME定义了scrapy项目的名称，scrapy会通过这个名称去识别项目，一般默认即可。
SPIDER_MODULES定义了爬虫的存放路径
NEWSPIDER_MODULE 定义了新爬虫的生成的存放路径

# Obey robots.txt rules  ROBOTSTXT_OBEY = False
ROBOT协议默认关闭即可ps: 虽然机器人协议禁止了，但实际开发中仍需要遵循网站爬虫规则，避免给网站带来过多的负担。
# See also autothrottle settings and docs  DOWNLOAD_DELAY = 3
下载延迟，通过定义每个请求之间的时间间隔，来避免触发网站的反爬虫机制
DEFAULT_REQUEST_HEADERS = &#123;      &#x27;accept&#x27;: &#x27;application/json, text/plain, */*&#x27;,      &#x27;accept-language&#x27;: &#x27;zh-CN,zh;q=0.9&#x27;,      &#x27;origin&#x27;: &#x27;https://movie.douban.com&#x27;,      &#x27;priority&#x27;: &#x27;u=1, i&#x27;,      &#x27;referer&#x27;: &#x27;https://movie.douban.com/explore&#x27;,      &#x27;sec-ch-ua&#x27;: &#x27;&quot;Chromium&quot;;v=&quot;9&quot;, &quot;Not?A_Brand&quot;;v=&quot;8&quot;&#x27;,      &#x27;sec-ch-ua-mobile&#x27;: &#x27;?0&#x27;,      &#x27;sec-ch-ua-platform&#x27;: &#x27;&quot;Windows&quot;&#x27;,      &#x27;sec-fetch-dest&#x27;: &#x27;empty&#x27;,      &#x27;sec-fetch-mode&#x27;: &#x27;cors&#x27;,      &#x27;sec-fetch-site&#x27;: &#x27;same-site&#x27;,      &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 SLBrowser/9.0.6.2081 SLBChan/103 SLBVPV/64-bit&#x27;,    &#125;
请求头，这个如果尝试过爬虫应该都不陌生，就是代码去访问网站时带的信息，具体怎么获取可以看我之前的文章，比较基础。
ITEM_PIPELINES = &#123;     &quot;douban.pipelines.DoubanPipeline&quot;: 300,  &#125;
重要，这是开启pipeline的代码，一定要开，不然pipelines就无法使用。300 是该 Pipeline 的优先级，数字越小优先级越高。这里我们设置为 300，表示优先处理该 Pipeline。
# Set settings whose default value is deprecated to a future-proof value  TWISTED_REACTOR = &quot;twisted.internet.asyncioreactor.AsyncioSelectorReactor&quot;  FEED_EXPORT_ENCODING = &quot;utf-8&quot;
REQUEST_FINGERPRINTER_IMPLEMENTATION 和 TWISTED_REACTOR，它们是 Scrapy 的内部配置，用于确保兼容性和性能优化。一般创建项目时默认生成，不需要改。然后在终端运行，或如我写的那样就可在pycharm直接运行，得到如下结果，
]]></content>
  </entry>
  <entry>
    <title>js逆向学习（一）</title>
    <url>/2025/04/24/js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[概念JS的作用：JavaScript, 是一门能够运行在浏览器上的脚本语言. 简称JS。简单来说可以处理前端的一些简单的业务逻辑和用户行为、网页事件的触发和监听。那么既然JS是可以运行在浏览器上的脚本. 并且, 我们知道本质上, 浏览器是执行HTML程序的. 那么如何在HTML中引入JS呢?

方案一：直接在html的页面 script 标签中编写js代码
方案二：将js代码写在js文件中，然后通过script标签的src属性进行引用

数据的基本类型JS虽然是一个脚本语言. 麻雀虽小, 五脏俱全. 在js中也是可以像其他编程语言一样. 声明变量, 条件判断, 流程控制等等. 我们先看一下JS中的数据类型在js中主要有这么几种数据类型
// 这是数字  let a =10  //这是字符串  let b =&quot;青樽对月的小屋&quot;  // 这是布尔值  let c =true  // 这是空值  let d = null  // 这是undefined  let e  console.log(a)  console.log(b)  console.log(c)  console.log(d)  console.log(e)
在js中声明变量用var来声明,当然现在也可以用let,效果一样的，并推荐使用let
在js中使用&#x2F;&#x2F; 来表示单行注释. 使用&#x2F;* *&#x2F;表示多行注释. 
let 变量名; // 创建变量, 此时该变量除了被赋值啥也干不了. let 变量名 = 值; // 创建一个变量, 并且有值. let 变量名 = 值1, 变量名2 = 值2, 变量名3 = 值3.....; // 一次创建多个变量.并都有值let 变量名1, 变量名2, 变量名3 = 值3;  // 创建多个变量. 并且只有变量3有值
数据类型转换数据类型转换共有两种，一种是显式类型，一种是隐式类型（就是直接在运算过程中自动转化，比如数字11+字符串“100”，两者数据类型不同但运算过程中会转换成相同数据类型）
// string -&gt; number  :  parseInt(字符串)var a = &quot;10086&quot;;a = parseInt(a);  // 变成整数console.log(a + 10); // 10096// number -&gt; string  : 数字.toString() 或者 数字 + &quot;&quot;var a = 100;var b = a.toString();var c = a + &quot;&quot;;  console.log(b);console.log(c);// number -&gt; string: 数字转化成16进制的字符串var m = 122;var n = m.toString(16);console.log(n);// 进制转换：十六进制的AB的十进制是多少var d = parseInt(&quot;AB&quot;, 16); // 171// 自动转换：弱类型中的变量会根据当前代码的需要,进行类型的自动隐式转化var box1 = 1 + true;// true 转换成数值,是1, false转换成数值,是0console.log(box1); // 2var box2 = 1 + &quot;200&quot;;console.log(box2); // ‘1200’ 原因是,程序中+的含义有2种,第一: 两边数值相加, 第二: 两边字符串拼接.但是在js中运算符的优先级中, 字符串拼接的优先级要高于整数    // 值的加减乘除,所以解析器优先使用了+号作为了字符串的拼接符号了,因为程序就需要+号两边都是字符串才能完成运算操作,因此1变成字符串了。最终的结果就是 &quot;1&quot; +&quot;200&quot;var box3 = 1 - &quot;200&quot;;console.log(box3); // -199;因为-号中表示的就是左边的数值减去右边的数值,因此程序就会要求&quot;200&quot;是数值,因此内部偷偷的转换了一下
字符串操作// split   正则分割,经常用于把字符串转换成数组var str = &quot;广东-深圳-南山&quot;;var ret = str.split(&quot;-&quot;);console.log( ret );// substr  截取var str = &quot;hello world&quot;;var ret = str.substr(0,3);console.log(ret); // hel// trim    移除字符串首尾空白var password = &quot;    ge llo   &quot;;var ret = password.trim();console.log(ret) // ge lloconsole.log(password.length); // 13console.log(ret.length);  // 6// 切片,当前方法支持使用负数代表倒数下标(从0开始)// slice(开始下标)   从开始位置切到最后// slice(开始下标,结束下标)  从开始下标切到指定位置之前var str = &quot;helloworld&quot;;var ret = str.slice(3,6); // 开区间,不包含结束下标的内容console.log(ret); // lowvar ret = str.slice(5);console.log(ret); // worldvar ret = str.slice(2,-1);console.log(ret); // lloworls.substring(start, end)  //字符串切割, 从start切割到ends.length  //字符串长度s.charAt(i) //第i索引位置的字符  s[i]s.indexOf(&#x27;xxx&#x27;)  //返回xxx的索引位置, 如果没有xxx. 则返回-1s.lastIndexOf(&quot;xxx&quot;) //返回xxx的最后一次出现的索引位置，如果没有xxx. 则返回-1s.toUpperCase() //转换成大写字母s.startsWith(&quot;xxx&quot;)  //判断是否以xxx开头s.charCodeAt(i) //某个位置的字符的asciiString.fromCharCode(ascii) //给出ascii 还原成正常字符
字符串正则// match  正则匹配// js中也存在正则,正则的使用符号和python里面是一样的//语法：/正则表达式主体/修饰符(可选)//修饰符：	//i:执行对大小写不敏感的匹配。	//g:执行全局匹配（查找所有匹配而非在找到第一个匹配后停止）。var str = &quot;我的电话是: 13312345678,你的电话: 13512345678&quot;;var ret = str.match(/\d&#123;11&#125;/g); // 匹配,提取数据console.log(ret);// replace  正则替换var str = &quot;我的电话是: 13512345678&quot;;var ret = str.replace(/(\d&#123;3&#125;)\d&#123;4&#125;(\d&#123;4&#125;)/,&quot;$1****$2&quot;); // 正则的捕获模式  $1$2表示的正则中第一个和第二个小括号捕获的内容console.log(ret);// search  正则查找,如果查找不到,则返回-1var str = &quot;hello&quot;;var ret = str.search(/l/);console.log(ret);
运算符（逻辑和三目运算符）//逻辑运算符  &amp;&amp;   //并且  and    两边的运算结果为true,最终结果才是true  ||  // 或者  or     两边的运算结果为false,最终结果才是false  !    //非    not    运算符的结果如果是true,则最终结果是false ,反之亦然. //条件运算符[三目运算符]	 //条件?true:false	 //例如:	      var age = 12;        var ret = age&gt;=18?&quot;成年&quot;:&quot;未成年&quot;;         console.log(ret);

流程控制语句
顺序结构(从上向下顺序执行)
分支结构
循环结构

之前我们学习的方式就是顺序执行，即代码的执行从上到下，一行行分别执行。
例如：
console.log(&quot;星期一&quot;);console.log(&quot;星期二&quot;);console.log(&quot;星期三&quot;);

分支结构
if 分支语句if(条件)&#123;     // 条件为true时,执行的代码   &#125;      if(条件)&#123;     // 条件为true时,执行的代码   &#125;else&#123;     // 条件为false时,执行的代码   &#125;      if(条件1)&#123;     // 条件1为true时,执行的代码   &#125;else if(条件2)&#123;     // 条件2为true时,执行的代码      &#125;....      &#125;else&#123;     // 上述条件都不成立的时候,执行的代码   &#125;
switch语句switch(条件)&#123;      case 结果1:           满足条件执行的结果是结果1时,执行这里的代码..           break;      case 结果2:      	   满足条件执行的结果是结果2时,执行这里的代码..      	   break;      .....      default:           条件和上述所有结果都不相等时,则执行这里的代码   &#125;switch(&#x27;a&#x27;):  case 1: //只会会执行case 1下面的xxx代码  	xxx  	break;  case 2:  	xxx  	break;  default:  	xxx  	break


1、switch比if else更为简洁
2、执行效率更高。switch…case会生成一个跳转表来指示实际的case分支的地址，而这个跳转表的索引号与switch变量的值是相等的。从而，switch…case不用像if…else那样遍历条件分支直到命中条件，而只需访问对应索引号的表项从而到达定位分支的目的。
3、到底使用哪一个选择语句，代码环境有关，如果是范围取值，则使用if else语句更为快捷；如果是确定取值，则使用switch是更优方案。

循环语句
while循环

while(循环的条件)&#123;   // 循环条件为true的时候,会执行这里的代码&#125;  

循环案例：
var count = 0while (count&lt;10)&#123;    console.log(count);    count++;&#125;


for循环

// 循环三要素for(1.声明循环的开始; 2.条件; 4. 循环的计数)&#123;   // 3. 循环条件为true的时候,会执行这里的代码&#125;for(循环的成员下标 in 被循环的数据)&#123;   // 当被循环的数据一直没有执行到最后下标,都会不断执行这里的代码&#125;   

循环案例：
// 方式1for (var i = 0;i&lt;10;i++)&#123;	console.log(i)&#125;// 方式2var arr = [111,222,333]for (var i in arr)&#123;    console.log(i,arr[i])&#125;


退出循环（break和continue）

for (var i = 0;i&lt;100;i++)&#123;          if (i===88)&#123;              continue  // 退出当次循环              // break  // 退出当前整个循环          &#125;          console.log(i)      &#125;

数组对象
数组方法

var arr = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;];// 内置属性console.log( arr.length );// 获取指定下标的成员console.log( arr[3] ); // Dconsole.log( arr[arr.length-1] ); // 最后一个成员// (1) pop()  出栈,删除最后一个成员作为返回值var arr = [1,2,3,4,5];var ret = arr.pop();console.log(arr); // [1, 2, 3, 4]console.log(ret); // 5// (2) push() 入栈,给数组后面追加成员var arr = [1,2,3,4,5];arr.push(&quot;a&quot;);console.log(arr); // [1, 2, 3, 4, 5, &quot;a&quot;]// (3) shift是将数组的第一个元素删除var arr = [1,2,3,4,5];arr.shift()console.log(arr); // [2, 3, 4, 5]// (4) unshift是将value值插入到数组的开始var arr = [1,2,3,4,5];arr.unshift(&quot;bobo&quot;)console.log(arr); // [&quot;bobo&quot;,1,2, 3, 4, 5]// (5) reverse() 反转排列var arr = [1,2,3,4,5];arr.reverse();console.log(arr); // [5, 4, 3, 2, 1]// (6) slice(开始下标,结束下标)  切片,开区间var arr = [1,2,3,4,5];console.log(arr.slice(1,3));// (7) concat() 把2个或者多个数组合并var arr1 = [1,2,3];var arr2 = [4,5,7];var ret = arr1.concat(arr2);console.log( ret );// (8) join()  把数组的每一个成员按照指定的符号进行拼接成字符串var str = &quot;广东-深圳-南山&quot;;var arr = str.split(&quot;-&quot;);console.log( arr ); // [&quot;广东&quot;, &quot;深圳&quot;, &quot;南山&quot;];var arr1 = [&quot;广东&quot;, &quot;深圳&quot;, &quot;南山&quot;];var str1 = arr1.join(&quot;-&quot;);console.log( str1 ); // 广东-深圳-南山       


遍历数组

var arr = [12,23,34]for (var i in arr)&#123;      console.log(i,arr[i])&#125;
]]></content>
  </entry>
  <entry>
    <title>js逆向学习（二）</title>
    <url>/2025/04/25/js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[tip 由于js逆向中需要对js有一定的了解，所以在正式尝试案例前需要对js的一些基础有一定了解，所以会用一定的篇幅去解释一些基础概念语句。
object对象object对象的基本操作 Object 的实例不具备多少功能，但对于在应用程序中存储和传输数据而言，它们确实是非常理想的选择。&#96;创建 Object 实例的方式有两种。
//var person = new Object()  person.name = &quot;cyt&quot;  person.age = 19  console.log(person)   //第二种方式是使用对象字面量表示法。对象字面量是对象定义的一种简写形式，目的在于简化创建包含大量属性的对象的过程。 var people = &#123;      name : &quot;zyj&quot;,      age : 19  &#125;  console.log(people)

object可以通过. 和 []来访问。

console.log(people.name)  console.log(person[&#x27;age&#x27;])

object可以通过for循环遍历

for (var attr in person)&#123;          console.log(attr,person[attr]);     &#125;
通过构造函数和 new 关键字实现面向对象编程，展示对象的实例化、属性赋值和方法调用。尽管在 ES6+ 中更推荐使用 class，但理解构造函数模式仍是掌握 JS 面向对象的基础。
function person(name,age) &#123;      this.name = name      this.age = age      this.eat = function()&#123;          console.log(this.name+&quot;正在吃饭&quot;)      &#125;  &#125;  var p1 = new person(&quot;jyh&quot;,20)  p1.eat()

json序列化和反序列化JSON：JavaScript 对象表示法，是一种轻量级的数据交换格式。易于人阅读和编写。

json是一种数据格式, 语法一般是{}或者[]包含起来
内部成员以英文逗号隔开,最后一个成员不能使用逗号!
可以是键值对,也可以是列表成员
json中的成员如果是键值对,则键名必须是字符串.而json中的字符串必须使用双引号圈起来&#x2F;&#x2F; json数据也可以保存到文件中,一般以”.json”结尾.var data = &#123;    name: &quot;xiaoming&quot;,    age: 22,    say: function()&#123;        alert(123);    &#125;&#125;;// 把对象转换成json字符串var ret = JSON.stringify(data);console.log(ret ); // &#123;&quot;name&quot;:&quot;xiaoming&quot;,&quot;age&quot;:22&#125;// 把json字符串转换成json对象var ret2 = JSON.parse(ret);console.log(ret2);

Date对象
创建Date对象

//方法1：不指定参数var nowd1=new Date(); //获取当前时间console.log(nowd1);console.log(nowd1.toLocaleString( ));//方法2：参数为日期字符串var d2=new Date(&quot;2004/3/20 11:12&quot;);console.log(d2.toLocaleString( ));var d3=new Date(&quot;04/03/20 11:12&quot;);console.log(d3.toLocaleString( ));


获取时间信息

获取日期和时间var date=new Date();date.getDate()();    getDate()                 获取日    getDay ()                 获取星期    getMonth ()               获取月（0-11）    getFullYear ()            获取完整年份    getHours ()               获取小时    getMinutes ()             获取分钟    getSeconds ()             获取秒    getMilliseconds ()        获取毫秒

Math对象// Math对象的内置方法// abs(x)  返回数值的绝对值var num = -10;console.log( Math.abs(num) ); // 10// ceil(x)  向上取整var num = 10.3;console.log( Math.ceil(num) ); // 11// floor(x) 向下取整var num = 10.3;console.log( Math.floor(num) ); // 10// max(x,y,z,...,n)console.log( Math.max(3,56,3) ); // 56// min(x,y,z,...,n)// random()  生成0-1随机数console.log( Math.random() );// 生成0-10之间的数值console.log( Math.random() * 10 );// round(x) 四舍五入// 生成0-10之间的整数console.log( Math.round( Math.random() * 10 ) );
JS中的函数（重点）声明函数// 函数的定义方式1function 函数名 (参数)&#123;    函数体;    return 返回值;&#125; /*功能说明：    可以使用变量、常量或表达式作为函数调用的参数    函数由关键字function定义    函数名的定义规则与标识符一致，大小写是敏感的    返回值必须使用return    */ //  函数的定义方式2    //用 Function 类直接创建函数的语法如下：var 函数名 = new Function(&quot;参数1&quot;,&quot;参数n&quot;,&quot;function_body&quot;);//虽然由于字符串的关系，第二种形式写起来有些困难，但有助于理解函数只不过是一种引用类型

函数调用//f(); ---&gt;OKfunction f()&#123;    console.log(&quot;hello&quot;)&#125;f() //-----&gt;OK


不同于python，js代码在运行时，会分为两大部分———检查装载 和 执行阶段。

检查装载阶段：会先检测代码的语法错误，进行变量、函数的声明
执行阶段：变量的赋值、函数的调用等，都属于执行阶段。


函数参数（1） 参数基本使用
// 位置参数function add(a,b)&#123;    console.log(a);    console.log(b);&#125;add(1,2)// 会自动忽略第三个add(1,2,3)//会只输出1add(1)// 默认参数// 默认参数  function stu_info(name,gender)&#123;      console.log(&quot;姓名：&quot;+name+&quot; 性别：&quot;+gender)  &#125;    stu_info(&quot;bobo&quot;,&quot;男&quot;)
函数返回值在函数体内，使用 return 语句可以设置函数的返回值。一旦执行 return 语句，将停止函数的运行，并运算和返回 return 后面的表达式的值。如果函数不包含 return 语句，则执行完函数体内每条语句后，返回 undefined 值。
function add(x,y) &#123;          return x,y      &#125;var ret = add(2,5);console.log(ret)


1、在函数体内可以包含多条 return 语句，但是仅能执行一条 return 语句
2、函数的参数没有限制，但是返回值只能是一个；如果要输出多个值，可以通过数组或对象进行设计。

作用域作用域是JavaScript最重要的概念之一。
JavaScript中，变量的作用域有全局作用域和局部作用域两种。

局部变量,是在函数内部声明,它的生命周期在当前函数被调用的时候, 当函数调用完毕以后,则内存中自动销毁当前变量
全局变量,是在函数外部声明,它的生命周期在当前文件中被声明以后就保存在内存中,直到当前文件执行完毕以后,才会被内存销毁掉

首先熟悉下var
var name = &quot;bobo&quot;; // 声明一个全局变量 name并赋值”bobo“name = &quot;张三&quot;;  // 对已经存在的变量name重新赋值 ”张三“console.log(name);var  gender = &quot;male&quot;var  gender = &quot;female&quot; // 原内存释放与新内存开辟，指针指向新开辟的内存console.log(gender)

  作用域案例：
 var num = 10; // 在函数外部声明的变量, 全局变量function func()&#123;  //千万不要再函数内部存在和全局变量同名的变量  num = 20; // 函数内部直接使用变量,则默认调用了全局的变量,&#125;func();console.log(&quot;全局num：&quot;,num);
匿名函数匿名函数，即没有变量名的函数。在实际开发中使用的频率非常高！也是学好JS的重点。
   // 匿名函数赋值变量    var foo = function () &#123;        console.log(&quot;这是一个匿名函数！&quot;)    &#125;;foo() //调用匿名函数   // 匿名函数的自执行   (function (x,y) &#123;        console.log(x+y);    &#125;)(2,3)  // 匿名函数作为一个高阶函数使用  function bar() &#123;     return function () &#123;         console.log(&quot;inner函数！&quot;)     &#125; &#125; bar()()     

闭包函数我们先看一段代码. 
let name = &quot;周杰伦&quot;;function chi()&#123;    name = &quot;吃掉&quot;;&#125;chi();console.log(name);

发现没有, 在函数内部想要修改外部的变量是十分容易的一件事. 尤其是全局变量. 这是非常危险的.  试想, 我写了一个函数. 要用到name, 结果被别人写的某个函数给修改掉了. 多难受. 
接下来. 我们来看一个案例:  
同时运行下面两组代码：
// 1号工具人.var name = &quot;猪猪&quot;setTimeout(function()&#123;    console.log(&quot;一号工具人:&quot;+name) // 一号工具人还以为是猪猪呢, 但是该变量是不安全的.&#125;, 5000);

// 2号工具人var name = &quot;小林子&quot;console.log(&quot;二号工具人&quot;, name);

两组代码是在同一个空间内执行的. 他们拥有相同的作用域. 此时的变量势必是非常非常不安全的. 那么如何来解决呢?  注意, 在js里. 变量是有作用域的. 也就是说一个变量的声明和使用是有范围的. 不是无限的. 这一点, 很容易验证.
function fn()&#123;    let love = &quot;爱呀&quot;&#125;fn()console.log(love)

直接就报错了.  也就是说. 在js里是有全局和局部的概念的. 
直接声明在最外层的变量就是全局变量. 所有函数, 所有代码块都可以共享的. 但是反过来就不是了. 在函数内和代码块内声明的变量. 它是一个局部变量. 外界是无法进行访问的. 我们就可以利用这一点来给每个工具人创建一个局部空间. 就像这样:
// 1号工具人.(function()&#123;    var name = &quot;猪猪&quot;;    setTimeout(function()&#123;        console.log(&quot;一号工具人:&quot;+name) // 一号工具人还以为是alex呢, 但是该变量是不安全的.    &#125;, 5000);&#125;)();// 二号工具人(function()&#123;    var name = &quot;小林子&quot;    console.log(&quot;二号工具人&quot;, name);&#125;)();

这样虽然解决了变量的冲突问题. 但是…我们想想. 如果在函数外面需要函数内部的一些东西来帮我进行相关操作怎么办…比如, 一号工具人要提供一个功能(加密). 外界要调用. 怎么办?  
// 1号工具人.let encrypt_tool = (function()&#123;    let log_msg = &#x27;开始加密......\n&#x27;    // 我是一个加密函数    let encrypt = function(data)&#123;  // 数据        console.log(log_msg) //打印日志信息（访问外部变量）        // 返回密文        return atob(data);    &#125;    // 外面需要用到这个功能啊. 你得把这个东东返回啊. 返回加密函数    return encrypt;&#125;)();//外部调用console.log(encrypt_tool(&#x27;i love you&#x27;));

注意了. 我们如果封装一个加密js包的时候. 好像还得准备出解密的功能. 并且, 不可能一个js包就一个功能吧. 那怎么办?  我们可以返回一个对象. 对象里面可以存放好多个功能. 而一些不希望外界触碰的功能. 就可以很好的保护起来. 
// 1号工具人.let encrypt_tool = (function()&#123;    let log_msg_1 = &#x27;开始加密......&#x27;    let log_msg_2 = &#x27;开始解密......&#x27;    // 我是一个加密函数    let encrypt = function(data)&#123;  // 被加密数据        console.log(log_msg_1) //打印日志信息（访问外部变量）        // 返回密文        return atob(data);    &#125;;    //解密函数    let decrypt = function(en_data)&#123;  // 加密后的数据        console.log(log_msg_2) //打印日志信息（访问外部变量）        // 返回解密后的原数据        return btoa(en_data);    &#125;;    // 外面需要用到这个功能啊. 你得把这个东东返回啊. 返回加密函数    return &#123;&#x27;encrypt&#x27;:encrypt,&#x27;decrypt&#x27;:decrypt&#125;;&#125;)();//外部调用en_data = encrypt_tool.encrypt(&#x27;i love you&#x27;);de_data = encrypt_tool.decrypt(en_data)console.log(en_data);console.log(de_data);



OK. 至此. 何为闭包? 上面这个就是闭包. 相信你百度一下就会知道. 什么内层函数使用外层函数变量. 什么让一个变量常驻内存.等等. 其实你细看. 它之所以称之为闭包~. 它是一个封闭的环境. 在内部. 自己和自己玩儿. 避免了对该模块内部的冲击和改动. 避免的变量之间的冲突问题. 
闭包的特点:

内层函数对外层函数变量的使用. 
会让变量常驻与内存.

变量提升(不正常现象)看以下代码, 或多或少会有些问题的.
function fn()&#123;    console.log(name);    var name = &#x27;cyt&#x27;;&#125;fn()

发现问题了么. 这么写代码, 在其他语言里. 绝对是不允许的. 但是在js里. 不但允许, 还能执行. 为什么呢?  因为在js执行的时候. 它会首先检测你的代码.  发现在代码中会有name使用. OK. 运行时就会变成这样的逻辑:
function fn()&#123;    var name;    console.log(name);    name = &#x27;cyt&#x27;;&#125;fn()console.log(a);

看到了么. 实际运行的时候和我们写代码的顺序可能会不一样….这种把变量提前到代码块第一部分运行的逻辑被称为变量提升. 这在其他语言里是绝对没有的. 并且也不是什么好事情. 正常的逻辑不应该是这样的. 那么怎么办?  在新的ES6中. 就明确了, 这样使用变量是不完善的. es6提出. 用let来声明变量. 就不会出现该问题了. 
function fn()&#123;    console.log(name);  // 直接报错, let变量不可以变量提升.    let name = &#x27;大马猴&#x27;; &#125;fn()

**结论一, 用let声明变量是新版本javascript提倡的一种声明变量的方案. ** 
let还有哪些作用呢?  
function fn()&#123;    // console.log(name);  // 直接报错, let变量不可以变量提升.    // let name = &#x27;大马猴&#x27;;    var name = &quot;周杰伦&quot;;    var name = &quot;王力宏&quot;;    console.log(name);&#125;fn()

显然一个变量被声明了两次. 这样也是不合理的. var本意是声明变量. 同一个东西. 被声明两次. 所以ES6规定. let声明的变量. 在同一个作用域内. 只能声明一次. 
function fn()&#123;    // console.log(name);  // 直接报错, let变量不可以变量提升.    // let name = &#x27;大马猴&#x27;;    let name = &quot;周杰伦&quot;;    console.log(name);    let name = &quot;王力宏&quot;;    console.log(name);&#125;fn()

注意, 报错是发生在代码检查阶段. 所以. 上述代码根本就执行不了. 
结论二, 在同一个作用域内. let声明的变量只能声明一次. 其他使用上和var没有差别
js包的导入（exports）类似Python中的模块导入
// functions.js文件// 加法函数function add(a, b) &#123;  return a + b;&#125;// 乘法函数function multiply(a, b) &#123;  return a * b;&#125;// 导出函数exports.add = add;exports.multiply = multiply;

// main.js// 导入 functions 模块const functions = require(&#x27;./functions&#x27;);// 使用导入的函数console.log(functions.add(2, 3));  // 输出: 5console.log(functions.multiply(4, 5));  // 输出: 20
]]></content>
  </entry>
  <entry>
    <title>js逆向学习（三）</title>
    <url>/2025/04/25/js%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[加密算法ps：本文默认阅读者配置了node.js运行环境

base64编码Base64编码，是由64个字符组成编码集：26个大写字母AZ，26个小写字母az，10个数字0~9，符号“+”与符号“&#x2F;”。Base64编码的基本思路是将原始数据的三个字节拆分转化为四个字节，然后根据Base64的对应表，得到对应的编码数据。
当原始数据凑不够三个字节时，编码结果中会使用额外的符号“&#x3D;”来表示这种情况。
base64测试base64编码示例：
import base64# 将原始数据转化为二进制/字节数据data = &quot;you&quot;.encode(&quot;utf-8&quot;)print(data)# 把字节转化成b64bs = base64.b64encode(data).decode()print(bs)bs = &quot;yo&quot;.encode(&quot;utf-8&quot;)# 把字节转化成b64print(base64.b64encode(bs).decode())# 猜测结果bs = &quot;y&quot;.encode(&quot;utf-8&quot;)# 把字节转化成b64print(base64.b64encode(bs).decode())

base64解码示例：
注意, base64编码处理后的字符串长度. 一定是4的倍数（因为Base64编码的基本思路是将原始数据的三个字节拆分转化为四个字节）. 如果在网页上看到有些密文的b64长度不是4的倍数. 会报错
import base64s = &quot;eW91&quot;ret = base64.b64decode(s)print(ret) #正确s = &quot;eW91eQ==&quot;ret = base64.b64decode(s)print(ret) #正确s = &quot;eW91eQ&quot;ret = base64.b64decode(s)print(ret) #报错，s不是4的倍数

如果不是4的倍数如何处理呢？解决思路. 使用&#x3D;填充为4的倍数即可
s = &quot;eW91eQ&quot;#填充为4的倍数s += (&quot;=&quot; * (4 - len(s) % 4))print(&quot;填充后&quot;, s)ret = base64.b64decode(s).decode()print(ret)

base64 编码的优点：

算法是编码，不是压缩，编码后只会增加字节数（一般是比之前的多1&#x2F;3，比如之前是3， 编码后是4）
算法简单，基本不影响效率
算法可逆，解码很方便，不用于私密传输。

js常见的加密方式
加密在前端开发和爬虫中是经常遇见的。掌握了加密、解密算法也是你从一个编程小白到大神级别质的一个飞跃。且加密算法的熟练和剖析也是很有助于帮助我们实现高效的js逆向。下述只把我们常用的加密方法进行总结。不去深究加密的具体实现方式。

常见的加密算法基本分为这几类，

线性散列算法（签名算法）MD5
对称性加密算法 AES DES
非对称性加密算法 RSA



]]></content>
  </entry>
  <entry>
    <title>软测学习</title>
    <url>/2025/05/12/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/transpancy.css</url>
    <content><![CDATA[/* 1. 容器对齐与间距 */
.layout>div:first-child:not(.recent-posts) {
    -webkit-align-self: flex-start;
    align-self: flex-start;
    -ms-flex-item-align: start;
    padding: 50px 40px;
}

/* 2. 侧边栏卡片宽度控制 */
#aside-content #card-toc .toc-content {
    width: 340px;
}

#aside-content #card-toc .toc-content .toc-link {
    font-size: 16px;
}


#aside-content .card-announcement .item-headline i {
    color: rgb(53, 183, 41);
}

/*è¿™é‡Œæ”¾å¤–æ¡†*/
/* body{
    cursor:url(https://cdn.custom-cursor.com/db/5006/32/arrow2836.png) , default!important;
    }
    /*è¿™é‡Œæ”¾å†…æ¡†*/
/* a,img,button{
    cursor: url(https://cdn.custom-cursor.com/db/5005/32/arrow2836.png) , pointer !important;
    } */ */
 

.aplayer .aplayer-list ol li.aplayer-list-light, .aplayer, #aside_content .card-widget, .layout>div:first-child:not(.recent-posts), .layout_post>#page, .layout_post>#post, .read-mode .layout_post>#post{
    /* ä»¥ä¸‹ä»£è¡¨ç™½è‰²é€æ˜Žåº¦ä¸º0.3 */
    background: rgba(255, 255, 255, 0.9);
    border:2px solid #0ff5ff;
    box-shadow: 1px 1px 10px #0ff5ff;
}



/* 目录子元素样式​ */
toc-child {
    font-size: 10px;
}

/* 1. 背景与整体布局 */
#post .post-copyright {
    background: rgba(231, 252, 253, 0.9);
}

/* 元数据样式 */
#post .post-copyright .post-copyright-meta {
    color: #21597d;
    font-weight: bold;
}

/* 正文内容样式 */
#post .post-copyright .post-copyright-info {
    padding-left: 6px;
    color: #5274ba;
}

/* 4. 链接样式 */
#post .post-copyright .post-copyright-info a {
    padding-left: 6px;
    color: #4b8de8
}

/* 页面容器样式​ */
#git_container,
#archive,
#page {
    background: rgba(214, 242, 244, 0.8);
    /* 半透明浅蓝背景 */
    border: 2px solid #0ff5ff !important;
    /* 青色边框 */
    box-shadow: 1px 1px 20px #0ff5ff !important;
    /* 浅蓝投影 */
}

/* ​​卡片组件样式 */
.card-widget {
    border: 2px solid #3d0f3f00 !important;
    /* 透明深灰边框 */
    box-shadow: 1px 1px 20px #c4e8fc !important;
    /* 浅蓝投影 */
    --font-color: #2e8086;
    /* 定义字体色变量 */
    --text-highlight-color: #110101;
    /* 定义高亮色变量 */
}

/* 暗色模式适配​ */
[data-theme='dark'] {
    --font-color: black;
    --text-highlight-color: #fdeacc;
}


标题样式​ #subtitle,
#site-title {
    color: #ffffff !important;
}



.layout_page>div:first-child:not(.recent-posts),
.layout_post>#page,
.layout_post>#post,
.read-mode .layout_post>#post {
    background: var(--light_bg_color)
}


#aside-content .card-info .author-info__name {
    font-weight: 500;
    font-size: 2.57em;
    color: #ffffff;
}

/* ç®€ä»‹ */
#aside-content .card-info .author-info__description {
    margin-top: -0.42em;
    font-size: 1.30em;
    color: #000000;
}


/* 边栏文章等 */
.site-data>a .headline {
    font-size: 20px;
    color: #21597d;
}

/* 边栏文章等的数字 */
.site-data>a .length-num {
    margin-top: -0.32em;
    color: var(--text-highlight-color);
    font-size: 1.4em;
}

/* å¾®ä¿¡ */
#aside-content .card-widget.card-announcement {
    background-color: #a2d1b2;
}

/* ç›®å½• */
#aside-content #card-toc {
    background-color: #f5e9eb;
}

/* æœ€è¿‘æ–‡ç«  */
#aside-content .card-widget.card-recent-post {
    background-color: #e2ebda;
}

/* ç½‘ç«™ä¿¡æ¯ */
#aside-content .card-widget.card-webinfo {
    background-color: #c7c7c6;
}

/* å¾®åšä¿¡æ¯ */
#weibo.card-widget {
    background: linear-gradient(to right, #ff5f6d, #ffc371);
    /* æ·»åŠ å…¶ä»–æ ·å¼ */
    border-radius: 10px;
    color: #0ff5ff ;
    border-width: 10px;
}



@-webkit-keyframes Gradient {
    0% {
        background-position: 0% 50%;
    }

    50% {
        background-position: 100% 50%;
    }

    100% {
        background-position: 0% 50%;
    }
}

@-moz-keyframes Gradient {
    0% {
        background-position: 0% 50%;
    }

    50% {
        background-position: 100% 50%;
    }

    100% {
        background-position: 0% 50%;
    }
}

@keyframes Gradient {
    0% {
        background-position: 0% 50%;
    }

    50% {
        background-position: 100% 50%;
    }

    100% {
        background-position: 0% 50%;
    }
}

/* ä¸ªäººä¿¡æ¯Follow meæŒ‰é’® */
#aside-content>.card-widget.card-info>#card-info-btn {
    background-color: #3eb8be;
    border-radius: 8px;
}


/* æ³¢æµª */
/* æ³¢æµªcss */
.main-hero-waves-area {
    width: 100%;
    position: absolute;
    left: 0;
    bottom: -11px;
    z-index: 5;
}

.waves-area .waves-svg {
    width: 100%;
    height: 5rem;
}

/* Animation */

.parallax>use {
    animation: move-forever 25s cubic-bezier(0.55, 0.5, 0.45, 0.5) infinite;
}

.parallax>use:nth-child(1) {
    animation-delay: -2s;
    animation-duration: 7s;
    fill: #f7f9febd;
}

.parallax>use:nth-child(2) {
    animation-delay: -3s;
    animation-duration: 10s;
    fill: #f7f9fe82;
}

.parallax>use:nth-child(3) {
    animation-delay: -4s;
    animation-duration: 13s;
    fill: #f7f9fe36;
}

.parallax>use:nth-child(4) {
    animation-delay: -5s;
    animation-duration: 20s;
    fill: #f7f9fe;
}

/* é»‘è‰²æ¨¡å¼èƒŒæ™¯ */
[data-theme="dark"] .parallax>use:nth-child(1) {
    animation-delay: -2s;
    animation-duration: 7s;
    fill: #18171dc8;
}

[data-theme="dark"] .parallax>use:nth-child(2) {
    animation-delay: -3s;
    animation-duration: 10s;
    fill: #18171d80;
}

[data-theme="dark"] .parallax>use:nth-child(3) {
    animation-delay: -4s;
    animation-duration: 13s;
    fill: #18171d3e;
}

[data-theme="dark"] .parallax>use:nth-child(4) {
    animation-delay: -5s;
    animation-duration: 20s;
    fill: #18171d;
}

/* 1. 波浪动画 */
@keyframes move-forever {
    0% {
        transform: translate3d(-90px, 0, 0);
    }

    100% {
        transform: translate3d(85px, 0, 0);
    }
}

/*Shrinking for mobile*/
@media (max-width: 768px) {
    .waves-area .waves-svg {
        height: 40px;
        min-height: 40px;
    }
}]]></content>
  </entry>
  <entry>
    <title>关于我</title>
    <url>/about/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>电影</title>
    <url>/movies/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>小说</title>
    <url>/novels/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>友情链接</title>
    <url>/link/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>音乐</title>
    <url>/music/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>tags</title>
    <url>/tags/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/css/custom.css</url>
    <content><![CDATA[/* 导航栏模块 */
/* 一级菜单居中 */
#nav .menus_items {
    position: absolute !important;
    width: fit-content !important;
    left: 50% !important;
    transform: translateX(-50%) !important;
    font-size: 25px !important; /* 独立于全局字体 */
      
}

/* 子菜单横向展示 */
#nav .menus_items .menus_item:hover .menus_item_child {
    display: row !important;
}

/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */
.menus_items .menus_item:nth-child(5) .menus_item_child {
    left: -50px;
}



/* 背景的通明度模块 */
/* 文章页背景 */
.layout_post>#post {
    /* 以下代表透明度为0.7 可以自行修改*/
    background: rgba(255,255,255,.9);

}
 
/* 所有页面背景 */
#aside_content .card-widget, #recent-posts>.recent-post-item, .layout_page>div:first-child:not(.recent-posts), .layout_post>#page, .layout_post>#post, .read-mode .layout_post>#post{
    /* 以下代表透明度为0.7 */
    background: rgba(255,255,255,.9);
    border:2px solid #0ff5ff;
    box-shadow: 1px 1px 10px #0ff5ff;
 
}
/* 侧边卡片的透明度 */
:root {
  --card-bg: rgba(255, 255, 255, .9);
}

/* 页脚透明 */
#footer {
	/* 以下代表透明度为0.7 */
	background: rgba(255,255,255, .0);
}
 

:root {
    --global-font-size: 20px;
}


.aplayer .aplayer-list ol li.aplayer-list-light, .aplayer, #aside_content .card-widget, #recent-posts>.recent-post-item, .layout>div:first-child:not(.recent-posts), .layout_post>#page, .layout_post>#post, .read-mode .layout_post>#post{
    /* ä»¥ä¸‹ä»£è¡¨ç™½è‰²é€æ˜Žåº¦ä¸º0.3 */
    background: rgba(255, 255, 255, 0.9);
    border:2px solid #0ff5ff;
    box-shadow: 1px 1px 10px #0ff5ff;
}






  
 .recent-post-item{

    background: rgba(255, 255, 255, 0.9);
    border:2px solid #0ff5ff;
    box-shadow: 1px 1px 10px #0fffc3;
}
]]></content>
  </entry>
  <entry>
    <title>醉欢行</title>
    <url>/%E9%86%89%E6%AC%A2%E8%A1%8C/index.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>水调歌头</title>
    <url>/poem/%E6%B0%B4%E8%B0%83%E6%AD%8C%E5%A4%B4.html</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>隐藏在时光里的回忆</title>
    <url>/%E5%9B%9E%E5%BF%86/index.html</url>
    <content><![CDATA[序 回忆璀璨如花嵊县的秋季格外凉爽，兴许是军训的后遗症，又或许是因为全球变暖让楚宇这颗心系世和平的心莫名的烦躁，趁老师不注意，趴在桌子上百无聊赖地望着窗外一片一片下落的银杏。
还没多看几眼便被一旁的男生用笔戳了戳腰，
“别发呆了，我看君姐已经瞥了你好几眼了，你是真喜欢上她办公室的龙井了？”
楚宇稍微起了起身子，给同桌翻了个白眼，露出手肘下面的翻在《荷塘月色》一面的语文书，
“啧，我这可不叫发呆，我只是在体会朱自清先生‘什么都可以想，什么都可以不想’的意境！还有，你这个开学没多久就因为上课看小说跟老师混熟的也没资格说我吧。”
说罢，楚宇还翘起嘴巴，用下巴点了点同桌本子下藏着的《花火》。
“但我成绩全班前三。”面对楚宇的挑衅，其同桌轻描淡写地答道。
楚宇忍住想要怒拍桌子的手，声音低沉地抗议，“难道成绩好就可以为所欲为吗!气抖冷，学渣什么时候才能站起来！”
同桌翻了翻白眼，道：“呵，站起来也给你腿打折，而且，成绩好确实可以为所欲为。”
楚宇刚要开口怼回去，就听得讲台上一阵河东狮吼传过：“楚宇，江耿给我站起来！楚宇，你来回答荷塘月色一共分为几部分，分别用几幅图来概括各部分内容。
楚宇回答不出来，江耿接着，要是都不会，这节课就站着吧。”
这两同桌一个发呆，一个看小说，最终结果自然是全军覆没。
当君姐重新开始上课时，两人不约而同的侧向对方，骂道：
“还不都是因为你！”
“还不都是因为你！”
……..
“向前跑！
迎着眼泪和嘲笑，
生命的广阔不经历风雨怎能感到…….”
一阵手机铃声将楚宇从睡梦中惊醒，楚宇左脚一踹，碰到守株待兔的墙壁，顿时缩进被窝里裹成史莱姆状，捂着脚指头瑟瑟发抖，将刚刚响铃抛之脑后。
但是电话铃却是不肯停歇，没一会又响了起来。
史莱姆状的被窝先是沉寂几秒钟，然后被色史莱姆缓缓向床头蠕动，伸出一只手，摸到床头的手机，看了看联系人，不耐烦地接到：“喂，徐正言！不知道放假人，人上人，扰人清梦可是大忌啊！”
“……”电话那先是一阵沉默，随即传来一阵怒吼，“你是不是忘了今天还要回学校！再不过来，老张就要来亲自问候您了！”
本还在撞墙以及起床气双重debuff中的楚宇听到“老张”二字突然清醒，瞥了一眼手机上的时间日期，顾不得留恋温暖的被窝，毫无形象地从床上爬起。
…………
紧赶慢赶，当楚宇冲进教室时，老张似乎还没到，同学们三三两两地坐一块，毕竟也算是高中毕业了，多数人都是把手机带来了。有王者开黑的，有聚在一块插科打诨的，嗯，也有刚毕业就斥巨资买了个索尼新款耳机然后就整天机不离手不知道搁那听啥的憨憨。
楚宇径直地走向自己座位，把同桌的耳机摘了下来，戴到了自己耳朵上，听着耳机中传来林肯公园的《In The End》，向徐正言问道：
“老张有没有来过？毕业典礼啥时候开始啊？”
兴许是对楚宇拿耳机的不爽，楚宇面对徐正言故意光张嘴不说话的幼稚行为，大声回怼：
“你刚电话里不说的顶凶？现在见面了不敢说话了？”
徐正言显然是不知道楚宇会这么回答，先是一怔，然后一把把耳机从楚宇头上摘下，
“他喵的你带着降噪耳机跟我说话你是想咋样啊喂，还我不敢说话，你怎么这么能啊！我……”
还没等徐正言骂完，班主任张德新出现在门口，走进教室，不得不说即使毕业了，班主任的威严是丝毫不减，班里瞬间鸦雀无声，玩手机的也不自觉的把手机藏了藏。徐正言白了楚宇一眼，也正了正身子。
“今天的流程十分简短，等你们听完毕业典礼，回来自己整完书就可以回去了。
去之前让我再唠叨几句，高考前，总是抓着你们念，高考是你们转折点，反复提醒，显得高考失败人生都输了似的，
现在肯定不会这么说，哪怕你高考失利，大学只要加把劲，考研也能考一个好学校……”
老班这段透着“主要唠叨会随当前情境改变”变相劝学的言论，不由得引起同学们的轻笑。
楚宇也放松起来，身子趴在桌上，看着讲台上还在念叨的老班，好笑又有些伤感的想到，
“都毕业了还是不忘催我们去学习啊。”
开学典礼，学生大会，结业典礼……三年来在学校参加的数不胜数的会典都是大同小异，校长演讲，表扬，鸡汤等等。无论是经历过学生时代，还是正处于青春对此想必都耳熟能详。
许是老张难得没敷衍我们，典礼流程确实简短，也有可能是有着手机打发时间，转眼间典礼就结束了，同学们闹哄哄地跑向教室，边跑边跟着一旁的同学吐槽，
“学校？我可是不能多待一秒！冲冲冲！”
徐正言也是手脚利落，三下五除二便把书整理好，见楚宇坐在位置上看着手机发呆，用力拍了一下楚宇的背，顺势将头一伸，
“跟哪个妹纸聊天呢？书都还没整完？”
但显然是楚宇熄屏键按得更快，待徐正言定睛看清，只能看到黑屏上倒映的自己。
楚宇没好气地说道：“就你整天爱吃瓜，我哪像你这么不爱学习书这么少，我爸妈还没到呢，咱整完书也没地放。”
徐正言对这种高考完一点书不整直接奔回家，现在还如此理直气壮的行为不做表示，向楚宇挥了挥手，留下句，“暑假聚”，转身离去，只给楚宇留下个后脑勺。
过了没多久，楚宇趁着父母没到，带着早上匆忙拿来的袋子也走出了教室。
…………
窗外树影婆娑，木槿摇曳，朦胧的月色星光不知被何人碾碎，织起一浣轻纱，平铺在天空之中。
夏日的夜晚仍是可以清凉的，楚宇坐在书桌前，感受着一阵阵微风拂来，看着桌上的日记本。
这是楚宇下午整理出来的，刚翻出来时，它混杂在一堆教科书作业本中，虽然高考前三个月完全忘记了日记这回事，将其埋没于书海中，1但意外地没有褶皱，安静地躺着，像是隐藏在时光中似的。
花了几个小时把整本日记浏览了一遍，然后在开头故意留出的空白的一页提笔写下，
回忆璀璨如花
第一章八月末的阳光格外的毒辣，原本清凉的早晨的微风也带着一丝燥热，几只飞鸟路过，穿过如今已愈发少见的喷气式飞机留下的痕迹，此时知了的精力最是旺盛，阳光透过树叶形成斑驳的小圆影，打在鸣叫的知了上，宛若为其穿上了新衣。
原本安逸的校园因为一些“不速之客”打破了此时祥和的氛围。
“砰！”
楚宇用自己的身体将虚掩着的门撞开，灰尘扬起，在阳光的照射下格外显眼，如同精灵般飞舞着。
拿着大包小包的楚宇平复着略微有点喘的呼吸，心里念叨着，
“嘁，出师不利，七点多就来学校不说，刚入学就让我爬六楼给我来个下马威是吧，以后放学已经不想回寝室了。”
若是能回到前一天晚上，他一定会把那个义正严词地拒绝了母亲想要来帮忙搬运行李的请求，拍着胸脯发誓一个人也能搞定的自己打一顿，然后哭求妈妈再爱我一次。
当然这些臆想也就一闪而过，毕竟还有一堆行李要整，他回过神仔细环顾了四周，将自己未来几年居住的寝室布局纳入眼帘：
抬头正对门的便是卫生间，储物柜在入门左手，兴许因为是c形楼最边上的寝室，与先前瞥到的别的寝室床、柜子、卫生间堆挤在一个房间的布局不同，楚宇寝室的储物柜与卫生间独立在一个隔间，右手的过道穿过个约一人半长宽的门形口子才到摆放着几张双人床的“住宿区”，过道的尽头便是阳台。
楚宇粗略地扫了一眼过道，虽然寝室许久没有人，但看起来还是蛮干净的。先前自己还担忧得废些力气才能打扫完工。
待楚宇拖着行李箱以及装着被褥的寝室袋进去寝室，才发现寝室这么干净是有原因的——有位男生在阳台死角上，似是听到动静，正好冒出个头，望向楚宇。
阳台的窗户开着，一阵微风拂过，吹起男生的碎发，那位男生似是双腿一蹬，楚宇便看到男生“咕噜，咕噜”的坐着行李箱划了出来，他从行李箱小跳了下来，面带微笑地走过来帮忙拿行李，说道：
“你也是601的吧？我叫江耿，将来几年就好好相处喽！”
“谢谢帮忙，额……我叫楚宇。”楚宇表面上没有异样，但心里完全任由吐槽本能爆发。
“啧啧，可恶啊，热情开朗还长得小帅，妥妥的人生赢家模板啊，唯一的不足是身高似乎只有170出头，嗯比我矮好些。”
楚宇自然也知道身高的比较只是毫无意义的逞强，但是男人的迷之胜负欲就是如此，孔子都说三人行必有我师，自己总得有些比得过人家的方面是吧。
虽说楚宇思绪越飘越远，但手上没闲着，找到自己靠阳台侧上铺的床位，将一些行李放在上面。
说来也巧，另一旁的上铺的被褥也已经整理好了，不用多想也能猜到这就是那位江耿同学的床位。
兴许是江耿在阳台的活也干完了，他并没有回阳台继续捣鼓，而是打开行李箱整理衣服。楚宇也开始了整理被褥、铺床，短短几分钟，601寝室便出现了安静—热闹—寂静的不同氛围，没错，你没看错就是寂静，若是江耿继续回阳台干活，楚宇倒也不会怎么样，但当其也在“住宿区”整理时，两者皆不说话的氛围就让楚宇有些待不住了，感觉气氛一度凝固。当楚宇快忍不住，准备找些垃圾话活跃活跃气氛时，江耿也发话了：
“学校规定的是九点半到校吧，这才七点出头，你为什么来的这么早？要不是尚东那的公交车来城里只有六点多的，我现在还睡着呢！”
闻言，楚宇也是暗松了口气，在听到江耿是尚东的后，也了然原因，楚宇父亲也是尚东镇的，偶尔自驾来回一趟便要花费不少时间，更别说是坐城乡公交车了，导致平日里那边公交车的排班确实不多。楚宇没放下手中的工作，边整理边回答：
“”
]]></content>
  </entry>
  <entry>
    <title>诗词</title>
    <url>/poem/index.html</url>
    <content><![CDATA[你是刷 Visa 還是 UnionPay


2021年快到了….


小心開車 安全至上


這是三片呢？還是四片？


你是刷 Visa


剪刀石頭布


前端最討厭的瀏覽器


]]></content>
  </entry>
</search>
